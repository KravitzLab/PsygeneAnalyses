{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/PsygeneAnalyses/blob/main/Kravitz_PR1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FED3 PR1 analysis\n",
        "<br>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqe_1a1j1bQaqhOxq0VvukPqfolLRUqOdl-g&s\" width=\"200\" />\n",
        "\n",
        "Updated: 10.13.25\n",
        "Authored by Chantelle Murrell and Sebastian Alves\n",
        "\n"
      ],
      "metadata": {
        "id": "hEm7saj7IRD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries and import them {\"run\":\"auto\"}\n",
        "\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Packages to ensure are installed (add others here if you like)\n",
        "packages = {\n",
        "    \"fed3\": \"git+https://github.com/earnestt1234/fed3.git\",\n",
        "    \"fed3bandit\": \"fed3bandit\",\n",
        "    \"pingouin\": \"pingouin\",\n",
        "    \"ipydatagrid\": \"ipydatagrid\",\n",
        "    \"openpyxl\": \"openpyxl\",\n",
        "}\n",
        "\n",
        "for name, source in packages.items():\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        print(f\"Installing {name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", source])\n",
        "\n",
        "# ----------------------------\n",
        "# Imports\n",
        "# ----------------------------\n",
        "# Standard library\n",
        "import copy\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Third-party\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import fed3\n",
        "import fed3.plot as fplot\n",
        "import fed3bandit as f3b\n",
        "from scipy.stats import f_oneway\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams.update({'font.size': 12, 'figure.autolayout': True})\n",
        "plt.rcParams['figure.figsize'] = [6, 4]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "print(\"Packages installed and imports ready.\")\n"
      ],
      "metadata": {
        "id": "k-YGEmW-KCNK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "73109545-2912-416b-c329-1b4358c403f7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing fed3...\n",
            "Installing fed3bandit...\n",
            "Installing pingouin...\n",
            "Installing ipydatagrid...\n",
            "Packages installed and imports ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload files\n",
        "\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "feds, loaded_files, session_types = [], [], []\n",
        "\n",
        "def extract_session_type(csv_path, fallback=\"Unknown\"):\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return fallback\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            for zi in zf.infolist():\n",
        "                if not zi.filename.lower().endswith(\".csv\"):\n",
        "                    continue\n",
        "                file_data = zf.read(zi)\n",
        "                if len(file_data) <= 1024:\n",
        "                    continue\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "                    tmp.write(file_data); tmp_path = tmp.name\n",
        "                try:\n",
        "                    session_type = extract_session_type(tmp_path)\n",
        "                    df = fed3.load(tmp_path)\n",
        "                    df.name = os.path.basename(zi.filename)\n",
        "                    df.attrs = {\"Session_type\": session_type}\n",
        "                    feds.append(df)\n",
        "                    loaded_files.append(os.path.basename(zi.filename))\n",
        "                    session_types.append(session_type)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {zi.filename}: {e}\")\n",
        "                finally:\n",
        "                    os.remove(tmp_path)\n",
        "    elif name.lower().endswith(\".csv\"):\n",
        "        if len(data) <= 1024:\n",
        "            continue\n",
        "        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".csv\", delete=False) as tmp:\n",
        "            tmp.write(data); tmp_path = tmp.name\n",
        "        try:\n",
        "            session_type = extract_session_type(tmp_path)\n",
        "            df = fed3.load(tmp_path)\n",
        "            df.name = os.path.basename(name)\n",
        "            df.attrs = {\"Session_type\": session_type}\n",
        "            feds.append(df)\n",
        "            loaded_files.append(os.path.basename(name))\n",
        "            session_types.append(session_type)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {name}: {e}\")\n",
        "        finally:\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "print(f\"Loaded {len(loaded_files)} files. Session types captured for all.\")\n",
        "# Optional quick plot\n",
        "if feds:\n",
        "    try:\n",
        "        fed3.as_aligned(feds, alignment=\"datetime\", inplace=True)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        fplot.line(feds, y='pellets'); plt.legend().remove(); plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting skipped: {e}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TONj_5IvKdNs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Key_Df, Display & Edit\n",
        "\n",
        "\n",
        "import os, glob, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipydatagrid import DataGrid, TextRenderer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import files as colab_files\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "# Require that the file-upload cell has already populated these:\n",
        "assert 'loaded_files' in globals() and 'session_types' in globals(), \\\n",
        "    \"Run the 'Upload FED3 files' cell first.\"\n",
        "\n",
        "colab_output.enable_custom_widget_manager()\n",
        "\n",
        "# ---------------------------\n",
        "# Base: bare-bones Key_Df from loaded data\n",
        "# ---------------------------\n",
        "def _make_base_key_df():\n",
        "    return pd.DataFrame({\"filename\": loaded_files, \"Session_type\": session_types})\n",
        "\n",
        "def _file_base(s):\n",
        "    return os.path.splitext(os.path.basename(str(s)))[0].strip()\n",
        "\n",
        "def _norm_base_lower(s):\n",
        "    return _file_base(s).lower()\n",
        "\n",
        "# ---------------------------\n",
        "# Key scanner: detect Mouse_ID or filename columns\n",
        "# ---------------------------\n",
        "def _scan_key_columns(df):\n",
        "    \"\"\"\n",
        "    Returns dict:\n",
        "      {\n",
        "        'has_mouse': bool,\n",
        "        'has_filename': bool,\n",
        "        'filename_col': 'filename'|'File'|None,\n",
        "        'msg': str\n",
        "      }\n",
        "    Accepts keys that have either Mouse_ID or a filename column (filename/File).\n",
        "    \"\"\"\n",
        "    info = {'has_mouse': False, 'has_filename': False, 'filename_col': None, 'msg': ''}\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        has_mouse = 'Mouse_ID' in cols\n",
        "        fname_col = 'filename' if 'filename' in cols else ('File' if 'File' in cols else None)\n",
        "        info.update({\n",
        "            'has_mouse': has_mouse,\n",
        "            'has_filename': fname_col is not None,\n",
        "            'filename_col': fname_col\n",
        "        })\n",
        "        if has_mouse:\n",
        "            info['msg'] = \"'Mouse_ID' found.\"\n",
        "        elif fname_col:\n",
        "            info['msg'] = f\"'{fname_col}' found; will match on filename.\"\n",
        "        else:\n",
        "            info['msg'] = \"Neither 'Mouse_ID' nor 'filename'/'File' found in provided key.\"\n",
        "    except Exception as e:\n",
        "        info['msg'] = f\"Error while checking key: {e}\"\n",
        "    return info\n",
        "\n",
        "# ---------------------------\n",
        "# Read uploaded key (CSV/XLSX), accept Mouse_ID or filename\n",
        "# ---------------------------\n",
        "def _read_key_from_upload(name, content_bytes):\n",
        "    \"\"\"Return (df_or_None, message). Reads CSV/XLSX bytes from Colab upload.\"\"\"\n",
        "    ext = name.lower().rsplit('.', 1)[-1] if '.' in name else ''\n",
        "    try:\n",
        "        bio = io.BytesIO(content_bytes)\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(bio, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(bio, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        # Normalize types/columns we might use later\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        # Persist a deterministic copy on disk for reproducibility\n",
        "        fixed_path = f\"_uploaded_key.{ext}\"\n",
        "        with open(fixed_path, \"wb\") as f:\n",
        "            f.write(content_bytes)\n",
        "        globals()['uploaded_key_path'] = fixed_path\n",
        "\n",
        "        return key_df, f\"Key loaded from upload ({name}) and saved to {fixed_path}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading uploaded key: {e}\"\n",
        "\n",
        "# ---------------------------\n",
        "# Matching filename <-> Mouse_ID\n",
        "# ---------------------------\n",
        "def _match_mouse_id_to_filenames(filenames, key_df):\n",
        "    \"\"\"Return DataFrame: filename, Mouse_ID, match_status based on Mouse_ID substring in filename.\"\"\"\n",
        "    base_names_lower = [_norm_base_lower(f) for f in filenames]\n",
        "    mouse_ids = (\n",
        "        key_df['Mouse_ID']\n",
        "        .dropna().astype(str).map(str.strip)\n",
        "        .replace({'': np.nan}).dropna().unique().tolist()\n",
        "    )\n",
        "    rows = []\n",
        "    for fname, base in zip(filenames, base_names_lower):\n",
        "        hits = [mid for mid in mouse_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": hits[0], \"match_status\": \"Matched (Mouse_ID in filename)\"})\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": best[0], \"match_status\": \"Matched (longest Mouse_ID token)\"})\n",
        "            else:\n",
        "                rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": f\"Ambiguous Mouse_ID: {hits}\"})\n",
        "        else:\n",
        "            rows.append({\"filename\": fname, \"Mouse_ID\": None, \"match_status\": \"Mouse_ID not found in filename\"})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------------\n",
        "# Build/Rematch function\n",
        "# ---------------------------\n",
        "status_box = widgets.Output()\n",
        "Key_Df = _make_base_key_df()  # start bare-bones\n",
        "\n",
        "def build_or_rematch_key_df(key_df=None, msg_hint=\"\"):\n",
        "    \"\"\"\n",
        "    If key_df provided and valid:\n",
        "      - Prefer Mouse_ID mapping if key has Mouse_ID.\n",
        "      - Else fall back to filename merge (case-insensitive basename).\n",
        "    Else: keep bare-bones.\n",
        "    \"\"\"\n",
        "    global Key_Df\n",
        "    files_df = _make_base_key_df().copy()\n",
        "    files_df['_file_base_lower'] = files_df['filename'].map(_norm_base_lower)\n",
        "\n",
        "    if key_df is None:\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"No key\"\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Key status: No key provided; showing bare-bones Key_Df.\")\n",
        "        return\n",
        "\n",
        "    # Identify key capabilities\n",
        "    scan = _scan_key_columns(key_df)\n",
        "    kd = key_df.copy()\n",
        "\n",
        "    # Make a unique version of the key for whichever join we use\n",
        "    def _dedup(df, subset_cols):\n",
        "        dup_counts = df[subset_cols].astype(str).agg('|'.join, axis=1).value_counts()\n",
        "        n_dups = int((dup_counts > 1).sum())\n",
        "        if n_dups:\n",
        "            with status_box:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Note: {n_dups} duplicate key(s) on {subset_cols}; taking the first occurrence.\")\n",
        "        return df.drop_duplicates(subset=subset_cols, keep=\"first\")\n",
        "\n",
        "    if scan['has_mouse']:\n",
        "        # Mouse_ID route (preferred)\n",
        "        kd['Mouse_ID'] = kd['Mouse_ID'].astype(str).str.strip()\n",
        "        key_unique = _dedup(kd, ['Mouse_ID'])\n",
        "\n",
        "        matched = _match_mouse_id_to_filenames(files_df['filename'].tolist(), key_unique)[\n",
        "            [\"filename\", \"Mouse_ID\", \"match_status\"]\n",
        "        ]\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(matched, on=\"filename\", how=\"left\")\n",
        "            .merge(key_unique, on=\"Mouse_ID\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower'])\n",
        "        )\n",
        "\n",
        "    elif scan['has_filename']:\n",
        "        # Filename route (fallback)\n",
        "        fcol = scan['filename_col']\n",
        "        kd['_key_file_base_lower'] = kd[fcol].map(_norm_base_lower)\n",
        "        key_unique = _dedup(kd, ['_key_file_base_lower'])\n",
        "\n",
        "        Key_Df = (\n",
        "            files_df\n",
        "            .merge(key_unique, left_on=\"_file_base_lower\", right_on=\"_key_file_base_lower\", how=\"left\", suffixes=(\"\", \"_key\"))\n",
        "            .drop(columns=['_file_base_lower', '_key_file_base_lower'])\n",
        "        )\n",
        "        # Give a simple match_status summary for filename matching\n",
        "        Key_Df[\"match_status\"] = np.where(\n",
        "            Key_Df[fcol].notna(), \"Matched (filename)\", \"Filename not found in key\"\n",
        "        )\n",
        "    else:\n",
        "        # Neither route available (shouldn't happen due to earlier check)\n",
        "        Key_Df = files_df.drop(columns=['_file_base_lower']).copy()\n",
        "        Key_Df[\"match_status\"] = \"Key missing Mouse_ID and filename columns\"\n",
        "\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if msg_hint:\n",
        "            print(msg_hint)\n",
        "        print(f\"Merged key columns into Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid UI\n",
        "# ---------------------------\n",
        "def make_grid(df: pd.DataFrame):\n",
        "    g = DataGrid(\n",
        "        df,\n",
        "        editable=True,\n",
        "        selection_mode='cell',\n",
        "        layout={'height': '420px'},\n",
        "        base_row_size=28,\n",
        "        base_column_size=120,\n",
        "    )\n",
        "    g.default_renderer = TextRenderer(text_wrap=True)\n",
        "    return g\n",
        "\n",
        "def rebuild_grid(msg=\"\"):\n",
        "    global grid, ui\n",
        "    df = Key_Df.copy().reset_index(drop=True)\n",
        "    new_grid = make_grid(df)\n",
        "    ui.children = (upload_row, new_grid, controls, status_box)\n",
        "    grid = new_grid\n",
        "    with status_box:\n",
        "        if msg:\n",
        "            print(msg)\n",
        "        print(f\"Grid now shows Key_Df ({len(df)} rows, {len(df.columns)} cols)\")\n",
        "\n",
        "# ---------------------------\n",
        "# Colab-native upload button ONLY (no path UI)\n",
        "# ---------------------------\n",
        "upload_btn = widgets.Button(description=\"Upload\", button_style=\"primary\", layout=widgets.Layout(width=\"120px\"))\n",
        "reset_btn = widgets.Button(description=\"Reset Key\", button_style=\"warning\", layout=widgets.Layout(width=\"120px\"))\n",
        "download_button = widgets.Button(description='Download', button_style='success', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def on_colab_upload(_):\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Opening Colab upload dialog...\")\n",
        "    uploaded = colab_files.upload()  # opens the native Colab picker\n",
        "    if not uploaded:\n",
        "        with status_box:\n",
        "            print(\"No file selected.\")\n",
        "        return\n",
        "    name, content = next(iter(uploaded.items()))\n",
        "    key_df, msg = _read_key_from_upload(name, content)\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg}\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid()\n",
        "\n",
        "def _load_saved_key_from_disk():\n",
        "    \"\"\"Returns (key_df_or_None, message) from uploaded_key_path if present/valid.\"\"\"\n",
        "    key_path = globals().get('uploaded_key_path', None)\n",
        "    if not (key_path and os.path.exists(key_path)):\n",
        "        return None, \"No saved key on disk to reload.\"\n",
        "    try:\n",
        "        ext = key_path.lower().rsplit('.', 1)[-1] if '.' in key_path else ''\n",
        "        if ext == 'xlsx':\n",
        "            xls = pd.ExcelFile(key_path, engine='openpyxl')\n",
        "            frames = [pd.read_excel(xls, sheet_name=s) for s in xls.sheet_names]\n",
        "            key_df = pd.concat(frames, ignore_index=True, sort=False)\n",
        "        elif ext == 'csv':\n",
        "            key_df = pd.read_csv(key_path, sep=None, engine='python')\n",
        "        else:\n",
        "            return None, f\"Unsupported key type .{ext}\"\n",
        "\n",
        "        key_df = key_df.copy()\n",
        "        key_df.columns = [str(c).strip() for c in key_df.columns]\n",
        "        scan = _scan_key_columns(key_df)\n",
        "        if not (scan['has_mouse'] or scan['has_filename']):\n",
        "            return None, scan['msg']\n",
        "\n",
        "        if scan['has_mouse']:\n",
        "            key_df['Mouse_ID'] = key_df['Mouse_ID'].astype(str).str.strip()\n",
        "        if scan['has_filename']:\n",
        "            fcol = scan['filename_col']\n",
        "            key_df[fcol] = key_df[fcol].astype(str).str.strip()\n",
        "            key_df['_key_file_base_lower'] = key_df[fcol].map(_norm_base_lower)\n",
        "\n",
        "        return key_df, f\"Key reloaded from {os.path.basename(key_path)}. {scan['msg']}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error reading saved uploaded key: {e}\"\n",
        "\n",
        "def on_reset(_):\n",
        "    \"\"\"\n",
        "    Reset now auto-rematches using the last uploaded key if available.\n",
        "    If no saved key exists or it's invalid, we fall back to bare-bones.\n",
        "    \"\"\"\n",
        "    key_df, msg = _load_saved_key_from_disk()\n",
        "    if key_df is None:\n",
        "        build_or_rematch_key_df(None)\n",
        "        rebuild_grid(f\"Key status: {msg} (showing bare-bones Key_Df).\")\n",
        "    else:\n",
        "        build_or_rematch_key_df(key_df, msg_hint=f\"Key status: {msg}\")\n",
        "        rebuild_grid(\"Reset: reloaded saved key and rematched.\")\n",
        "\n",
        "def download_df(_):\n",
        "    global Key_Df\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Saving latest Key_Df as XLSX ...\")\n",
        "    try:\n",
        "        path = \"/content/Key_Df.xlsx\"\n",
        "        Key_Df.to_excel(path, index=False, engine='openpyxl')\n",
        "        colab_files.download(path)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Saved and downloading Key_Df.xlsx ...\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error while saving/downloading: {e}\")\n",
        "\n",
        "upload_btn.on_click(on_colab_upload)\n",
        "reset_btn.on_click(on_reset)\n",
        "download_button.on_click(download_df)\n",
        "\n",
        "upload_row = widgets.HBox([\n",
        "    widgets.HTML(\"<b>Optional key:</b>\"),\n",
        "    upload_btn,\n",
        "    reset_btn,\n",
        "    download_button\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Edit / rematch / download controls\n",
        "# ---------------------------\n",
        "new_col_name    = widgets.Text(placeholder='Enter new column name', description='New Col:')\n",
        "add_col_button  = widgets.Button(description='Add Column', button_style='info')\n",
        "apply_button    = widgets.Button(description='Apply Changes', button_style='primary', layout=widgets.Layout(width=\"120px\"))\n",
        "\n",
        "def add_column(_):\n",
        "    global Key_Df\n",
        "    col = new_col_name.value.strip()\n",
        "    with status_box:\n",
        "        clear_output(wait=True)\n",
        "        if not col:\n",
        "            print(\"Please enter a column name.\"); return\n",
        "        if col in Key_Df.columns:\n",
        "            print(f\"Column '{col}' already exists.\"); return\n",
        "        Key_Df[col] = \"\"\n",
        "        print(f\"Added column '{col}' to Key_Df.\")\n",
        "    rebuild_grid()\n",
        "\n",
        "def apply_edits(_):\n",
        "    global Key_Df\n",
        "    try:\n",
        "        Key_Df = grid.data.copy().reset_index(drop=True)\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Applied grid edits to Key_Df ({len(Key_Df)} rows, {len(Key_Df.columns)} cols).\")\n",
        "    except Exception as e:\n",
        "        with status_box:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Error applying edits: {e}\")\n",
        "\n",
        "add_col_button.on_click(add_column)\n",
        "apply_button.on_click(apply_edits)\n",
        "\n",
        "controls = widgets.HBox([new_col_name, add_col_button, apply_button])\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize UI\n",
        "# ---------------------------\n",
        "Key_Df = _make_base_key_df()\n",
        "Key_Df[\"match_status\"] = \"No key\"\n",
        "\n",
        "grid = make_grid(Key_Df.copy().reset_index(drop=True))\n",
        "ui = widgets.VBox([upload_row, grid, controls, status_box])\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "_6ugA24nNdd2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Individual plots\n",
        "\n",
        "assert 'Key_Df' in globals() and isinstance(Key_Df, pd.DataFrame), \"Build/rematch Key_Df first.\"\n",
        "\n",
        "# metadata_df = copy of Key_Df\n",
        "metadata_df = Key_Df.copy().reset_index(drop=True)\n",
        "if 'filename' in metadata_df.columns:\n",
        "    metadata_df['filename'] = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "\n",
        "# -------- Plotting Function --------\n",
        "def plot_file(file_index):\n",
        "    \"\"\"\n",
        "    Returns (fig, filename) for UI to display/save.\n",
        "    Prints a message and returns (None, None) on errors.\n",
        "    \"\"\"\n",
        "    # Safety check\n",
        "    if 'feds' not in globals() or file_index >= len(feds):\n",
        "        print(f\"Index {file_index} is out of range (max {len(feds)-1 if 'feds' in globals() else 'N/A'}).\")\n",
        "        return None, None\n",
        "\n",
        "    df = feds[file_index]\n",
        "    # Keep only pellet rows, but don't mutate df\n",
        "    pellet_df = df[df['Event'] == 'Pellet'].copy()\n",
        "    if pellet_df.empty:\n",
        "        print(f\"No pellet events for file index {file_index}.\")\n",
        "        return None, None\n",
        "\n",
        "    # Determine filename for title/return\n",
        "    if 'loaded_files' in globals() and len(loaded_files) > file_index:\n",
        "        raw_file = loaded_files[file_index]\n",
        "    elif 'files' in globals() and len(files) > file_index:\n",
        "        raw_file = files[file_index]\n",
        "    else:\n",
        "        raw_file = f\"file_{file_index}\"\n",
        "    filename = os.path.basename(str(raw_file))\n",
        "\n",
        "    # Match Mouse_ID from metadata, robust to path vs basename\n",
        "    title_str = filename\n",
        "    if 'filename' in metadata_df.columns:\n",
        "        md_fn = metadata_df['filename'].astype(str).map(os.path.basename)\n",
        "        hit = metadata_df.loc[md_fn == filename]\n",
        "        if not hit.empty:\n",
        "            match_row = hit.iloc[0]\n",
        "            if 'Mouse_ID' in match_row and pd.notna(match_row['Mouse_ID']):\n",
        "                title_str = f\"{match_row['Mouse_ID']}\"\n",
        "\n",
        "    # Determine x-axis\n",
        "    x_series = None\n",
        "    x_is_datetime = isinstance(pellet_df.index, pd.DatetimeIndex)\n",
        "    if x_is_datetime:\n",
        "        x_series = pellet_df.index\n",
        "    else:\n",
        "        for col in ['Timestamp', 'Time', 'DateTime', 'Datetime', 'datetime']:\n",
        "            if col in pellet_df.columns:\n",
        "                ts = pd.to_datetime(pellet_df[col], errors='coerce')\n",
        "                if ts.notna().any():\n",
        "                    pellet_df['_x'] = ts\n",
        "                    x_series = pellet_df['_x']\n",
        "                    x_is_datetime = True\n",
        "                    break\n",
        "        if x_series is None:\n",
        "            x_series = pellet_df.index  # fall back to index\n",
        "\n",
        "    # Build figure (DO NOT show here)\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    hue_vals = pellet_df['Block_Pellet_Count'].clip(upper=40)\n",
        "    sns.scatterplot(\n",
        "        data=pellet_df,\n",
        "        x=x_series,\n",
        "        y='Block_Pellet_Count',\n",
        "        hue=hue_vals,\n",
        "        palette='spring',\n",
        "        alpha=0.6,\n",
        "        legend=False,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    # Night shading if datetime x\n",
        "    if x_is_datetime:\n",
        "        import datetime as dt\n",
        "        night_start = dt.time(18, 0)  # 18:00\n",
        "        night_end   = dt.time(6, 0)   # 06:00\n",
        "\n",
        "        start_date = pd.to_datetime(pd.Series(x_series)).min().normalize()\n",
        "        end_date   = pd.to_datetime(pd.Series(x_series)).max().normalize()\n",
        "\n",
        "        # shade from each day's 18:00 to next day's 06:00\n",
        "        for day in pd.date_range(start_date, end_date - pd.Timedelta(days=1)):\n",
        "            start = pd.Timestamp.combine(day, night_start)\n",
        "            end   = pd.Timestamp.combine(day + pd.Timedelta(days=1), night_end)\n",
        "            ax.axvspan(start, end, color='gray', alpha=0.2)\n",
        "\n",
        "        import matplotlib.dates as mdates\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
        "        ax.set_xlabel('')\n",
        "    else:\n",
        "        ax.set_xlabel('Index')\n",
        "    ax.set_ylabel('Pellet Count in Block')\n",
        "    ax.set_title(title_str)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Return for UI to display/save later\n",
        "    return fig, filename\n",
        "\n",
        "# ---------- UI: single instance ----------\n",
        "demand_ui = {}\n",
        "\n",
        "def _sanitize_filename(s: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)\n",
        "\n",
        "# Slider bounds from number of feds (not files)\n",
        "n_items = len(feds) if 'feds' in globals() else 1\n",
        "slider = widgets.IntSlider(\n",
        "    min=0, max=max(0, n_items-1), step=1, value=0,\n",
        "    description=\"File\", continuous_update=True\n",
        ")\n",
        "save_btn = widgets.Button(description=\"Save PDF\")\n",
        "status = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _update_plot(change=None):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        fig, file = plot_file(slider.value)\n",
        "        if fig is None:\n",
        "            status.value = \"<span style='color:#b00'>No figure to display.</span>\"\n",
        "            return\n",
        "        display(fig)      # show exactly one figure\n",
        "        plt.close(fig)    # close backend handle to prevent accumulation\n",
        "        demand_ui[\"_last_fig\"] = fig\n",
        "        demand_ui[\"_last_file\"] = file\n",
        "        status.value = \"\" # clear old status\n",
        "\n",
        "def _save_pdf(_):\n",
        "    fig = demand_ui.get(\"_last_fig\")\n",
        "    file = demand_ui.get(\"_last_file\", \"figure\")\n",
        "    if fig is None:\n",
        "        status.value = \"<span style='color:#b00'>No figure to save.</span>\"\n",
        "        return\n",
        "    base = _sanitize_filename(os.path.splitext(os.path.basename(str(file)))[0])\n",
        "    fname = f\"demand_{base}_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "    fig.savefig(fname, format=\"pdf\", bbox_inches=\"tight\")\n",
        "    if 'gfiles' in globals() and gfiles is not None:\n",
        "        status.value = f\"Preparing download: <code>{fname}</code>â€¦\"\n",
        "        try:\n",
        "            gfiles.download(fname)\n",
        "        except Exception as e:\n",
        "            status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code> (download helper failed: {e}).\"\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code>.\"\n",
        "\n",
        "slider.observe(_update_plot, names=\"value\")\n",
        "save_btn.on_click(_save_pdf)\n",
        "\n",
        "# Keep references so we can close them next run\n",
        "box = widgets.HBox([slider, save_btn])\n",
        "demand_ui.update({\"slider\": slider, \"save_btn\": save_btn, \"status\": status, \"out\": out, \"box\": box})\n",
        "\n",
        "# Display the controls and initial plot\n",
        "display(box, status, out)\n",
        "_update_plot()\n"
      ],
      "metadata": {
        "id": "PCXLxmMa-QXD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Analyse PR metrics\n",
        "from scipy.optimize import curve_fit\n",
        "from tqdm.auto import tqdm\n",
        "try:\n",
        "    from tqdm.auto import tqdm as _tqdm\n",
        "except Exception:\n",
        "    try:\n",
        "        # If user did `from tqdm import tqdm` earlier\n",
        "        _tqdm = tqdm  # type: ignore[name-defined]\n",
        "    except Exception:\n",
        "        # Fallback: no-op iterator\n",
        "        def _tqdm(x, **kwargs):\n",
        "            return x\n",
        "# --- Preconditions ---\n",
        "assert 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame), \\\n",
        "    \"metadata_df not found. Build/rematch Key_Df and set metadata_df = Key_Df.copy() first.\"\n",
        "assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \\\n",
        "    \"No FED3 sessions found. Expecting a non-empty 'feds' list.\"\n",
        "\n",
        "# ---------- Small helpers (merged + minimal PR utilities) ----------\n",
        "def _basename(pathlike) -> str:\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def _to_num(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "def _count_events(df, label: str) -> int:\n",
        "    if \"Event\" not in df.columns:\n",
        "        return 0\n",
        "    return int((df[\"Event\"].astype(str) == label).sum())\n",
        "\n",
        "def breakpoint_and_runs(df):\n",
        "    \"\"\"\n",
        "    Merge of _breakpoint_stats(df) and _count_runs(df).\n",
        "    Returns: (median_breakpoint, max_breakpoint, runs_count)\n",
        "    - 'Breakpoint' is taken at the end of a block/run: current BPC > 0 and next BPC == 0\n",
        "    - 'runs_count' is the number of such end-of-block events\n",
        "    \"\"\"\n",
        "    if \"Block_Pellet_Count\" not in df.columns or df[\"Block_Pellet_Count\"].empty:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    bpc = _to_num(df[\"Block_Pellet_Count\"])\n",
        "    # end-of-block: positive now, zero next\n",
        "    end_mask = (bpc > 0) & (bpc.shift(-1) == 0)\n",
        "    end_vals = bpc[end_mask]\n",
        "\n",
        "    med_bp = float(end_vals.median()) if len(end_vals) else np.nan\n",
        "    max_bp = float(bpc.max()) if bpc.notna().any() else np.nan\n",
        "    runs   = int(end_mask.sum())\n",
        "    return med_bp, max_bp, runs\n",
        "\n",
        "def _get_timestamp_series(df, ts_col=\"MM:DD:YYYY hh:mm:ss\"):\n",
        "    \"\"\"Return a pandas Series of timestamps (never an Index).\"\"\"\n",
        "    if ts_col in df.columns:\n",
        "        ts = pd.to_datetime(df[ts_col], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
        "        return pd.Series(ts, index=df.index)\n",
        "    for cand in [\"DateTime\", \"Datetime\", \"Timestamp\", \"timestamp\", \"datetime\"]:\n",
        "        if cand in df.columns:\n",
        "            ts = pd.to_datetime(df[cand], errors=\"coerce\")\n",
        "            return pd.Series(ts, index=df.index)\n",
        "    idx = df.index\n",
        "    if isinstance(idx, pd.DatetimeIndex):\n",
        "        return pd.Series(idx, index=df.index)\n",
        "    return pd.to_datetime(pd.Series(idx, index=df.index), errors=\"coerce\")\n",
        "\n",
        "def _estimate_daily_pellets(df):\n",
        "    ts = _get_timestamp_series(df)\n",
        "    valid_ts = ts.dropna()\n",
        "    if valid_ts.size < 2:\n",
        "        return np.nan\n",
        "    duration_hours = (valid_ts.max() - valid_ts.min()).total_seconds() / 3600.0\n",
        "    if duration_hours <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    pellet_events = np.nan\n",
        "    if \"Pellet_Count\" in df.columns and df[\"Pellet_Count\"].notna().any():\n",
        "        pc = pd.to_numeric(df[\"Pellet_Count\"], errors=\"coerce\")\n",
        "        if pc.notna().any():\n",
        "            diffs = pc.diff().fillna(0).clip(lower=0)\n",
        "            pellet_events = float(diffs.sum())\n",
        "            if pellet_events == 0 and pc.iloc[-1] >= pc.iloc[0]:\n",
        "                pellet_events = float(pc.iloc[-1] - pc.iloc[0])\n",
        "\n",
        "    if (pd.isna(pellet_events)) and (\"Event\" in df.columns):\n",
        "        pellet_events = float((df[\"Event\"].astype(str) == \"Pellet\").sum())\n",
        "\n",
        "    if pd.isna(pellet_events):\n",
        "        return np.nan\n",
        "    return (pellet_events / duration_hours) * 24.0\n",
        "\n",
        "# ---------- Step 4: normalize metadata_df ----------\n",
        "md = metadata_df.copy()\n",
        "# Normalize filename column to basename; accept either 'filename' or 'File'\n",
        "if 'filename' in md.columns:\n",
        "    md['filename'] = md['filename'].astype(str).map(_basename)\n",
        "elif 'File' in md.columns:\n",
        "    md['filename'] = md['File'].astype(str).map(_basename)\n",
        "else:\n",
        "    raise ValueError(\"metadata_df must contain a 'filename' or 'File' column.\")\n",
        "\n",
        "# Ensure Mouse_ID exists and is a clean string column\n",
        "if 'Mouse_ID' in md.columns:\n",
        "    md['Mouse_ID'] = md['Mouse_ID'].astype(str).str.strip()\n",
        "else:\n",
        "    md['Mouse_ID'] = np.nan\n",
        "\n",
        "# ---------- Step 5: compute PR metrics for ALL files ----------\n",
        "rows = []\n",
        "for idx, c_df in enumerate(_tqdm(feds, desc=\"Computing PR metrics\")):\n",
        "    # Prefer DataFrame.name if present; otherwise synthesize a filename\n",
        "    file_name = _basename(getattr(c_df, \"name\", f\"File_{idx}\"))\n",
        "\n",
        "    # Core PR metrics (Pellet/Left/Right/Total/Accuracy/PokesPerPellet)\n",
        "    pellets = _count_events(c_df, \"Pellet\")\n",
        "    left    = _count_events(c_df, \"Left\")\n",
        "    right   = _count_events(c_df, \"Right\")\n",
        "    total   = left + right\n",
        "    acc     = (left / total * 100.0) if total > 0 else np.nan\n",
        "    ppp     = (total / pellets) if pellets > 0 else np.nan\n",
        "\n",
        "    # Breakpoints + number of runs (merged helper)\n",
        "    med_bp, max_bp, runs = breakpoint_and_runs(c_df)\n",
        "\n",
        "    # Daily pellets estimate\n",
        "    daily  = _estimate_daily_pellets(c_df)\n",
        "\n",
        "    rows.append({\n",
        "        \"filename\": file_name,\n",
        "        \"Left_Poke\": left,\n",
        "        \"Right_Poke\": right,\n",
        "        \"Total_Pokes\": total,\n",
        "        \"Accuracy\": acc,\n",
        "        \"PokesPerPellet\": ppp,\n",
        "        \"MedianBreakPoint\": med_bp,\n",
        "        \"Numberofblocks\": runs,\n",
        "        \"daily pellets\": daily,\n",
        "    })\n",
        "\n",
        "PRmetrics = pd.DataFrame(rows)\n",
        "if PRmetrics.empty:\n",
        "    display(HTML(\"<b style='color:#b00'>No files to analyze.</b>\"))\n",
        "    raise SystemExit\n",
        "\n",
        "# keep this threshold; print when skipping\n",
        "MIN_RUNS_PER_MOUSE = 10  # \"runs\" == Numberofblocks\n",
        "\n",
        "# filename -> df (match how you named files earlier)\n",
        "def _basename(pathlike):\n",
        "    s = str(pathlike).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "file_names = [_basename(getattr(df, \"name\", f\"File_{i}\")) for i, df in enumerate(feds)]\n",
        "file_to_df = dict(zip(file_names, feds))\n",
        "\n",
        "# filename -> Mouse_ID mapping from normalized md (we haven't merged it yet, but md is ready)\n",
        "fname_to_mouse = md.set_index(\"filename\")[\"Mouse_ID\"].to_dict()\n",
        "\n",
        "# total runs (blocks) per mouse from PRmetrics + md mapping\n",
        "runs_tbl = (\n",
        "    PRmetrics[[\"filename\", \"Numberofblocks\"]]\n",
        "    .assign(Mouse_ID=lambda d: d[\"filename\"].map(fname_to_mouse))\n",
        ")\n",
        "runs_tbl[\"Numberofblocks\"] = pd.to_numeric(runs_tbl[\"Numberofblocks\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "runs_per_mouse = runs_tbl.groupby(\"Mouse_ID\", dropna=True)[\"Numberofblocks\"].sum().to_dict()\n",
        "keep_mice = {m for m, r in runs_per_mouse.items() if r >= MIN_RUNS_PER_MOUSE}\n",
        "for m, r in runs_per_mouse.items():\n",
        "    if m not in keep_mice:\n",
        "        print(f\"[{m}] skipped: only {r} blocks (< {MIN_RUNS_PER_MOUSE}).\")\n",
        "\n",
        "# build raw demand points per file -> roll up per mouse (no truncation, no min-bins)\n",
        "raw_rows = []\n",
        "for fn, df in file_to_df.items():\n",
        "    mouse = fname_to_mouse.get(fn, None)\n",
        "    if (mouse is None) or (mouse not in keep_mice):\n",
        "        continue\n",
        "    if {\"Event\", \"Block_Pellet_Count\"} - set(df.columns):\n",
        "        print(f\"[{mouse}] file {fn} missing Event/BPC columns; skipped.\")\n",
        "        continue\n",
        "\n",
        "    pellets = df[df[\"Event\"].astype(str) == \"Pellet\"].copy()\n",
        "    if pellets.empty:\n",
        "        print(f\"[{mouse}] file {fn} has 0 pellet rows; skipped.\")\n",
        "        continue\n",
        "\n",
        "    bpc = pd.to_numeric(pellets[\"Block_Pellet_Count\"], errors=\"coerce\")\n",
        "    counts = (\n",
        "        pellets.assign(Block_Pellet_Count=bpc)\n",
        "        .dropna(subset=[\"Block_Pellet_Count\"])\n",
        "        .groupby(\"Block_Pellet_Count\")\n",
        "        .size()\n",
        "    )\n",
        "    if counts.empty:\n",
        "        print(f\"[{mouse}] file {fn} produced no valid price bins; skipped.\")\n",
        "        continue\n",
        "\n",
        "    for price, cnt in counts.items():\n",
        "        raw_rows.append({\"Mouse_ID\": mouse, \"PricePaid\": int(price), \"PelletCount\": int(cnt)})\n",
        "\n",
        "demand_raw_df = pd.DataFrame(raw_rows)\n",
        "\n",
        "# if nothing, set up empty demand metrics; otherwise fit\n",
        "if demand_raw_df.empty:\n",
        "    demand_metrics = pd.DataFrame(columns=[\n",
        "        \"Mouse_ID\", \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "        \"Demand_alpha_norm\", \"Demand_beta_norm\"\n",
        "    ])\n",
        "else:\n",
        "    # collapse to per-mouse histogram\n",
        "    demand_mouse = (demand_raw_df\n",
        "                    .groupby([\"Mouse_ID\", \"PricePaid\"], as_index=False)[\"PelletCount\"]\n",
        "                    .sum())\n",
        "\n",
        "    # normalization factor per mouse (baseline price -> Q=100)\n",
        "    def _baseline_B(sub):\n",
        "        if sub.empty:\n",
        "            return np.nan\n",
        "        mprice = sub[\"PricePaid\"].min()\n",
        "        return sub.loc[sub[\"PricePaid\"] == mprice, \"PelletCount\"].mean()\n",
        "\n",
        "    B = (demand_mouse.groupby(\"Mouse_ID\")\n",
        "         .apply(_baseline_B).rename(\"B\").reset_index())\n",
        "    B[\"q\"] = 100.0 / B[\"B\"]\n",
        "\n",
        "    # Q-only normalization (keep FR requirement unscaled)\n",
        "    normalized = (\n",
        "        demand_mouse\n",
        "        .merge(B[[\"Mouse_ID\", \"q\"]], on=\"Mouse_ID\", how=\"left\")\n",
        "        .assign(\n",
        "            P_FR=lambda d: d[\"PricePaid\"],             # FR requirement (unscaled)\n",
        "            Q_norm=lambda d: d[\"PelletCount\"] * d[\"q\"] # normalize only quantity\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # models\n",
        "    def loglog3(P, Q0, a, b):  # raw, free Q0\n",
        "        return Q0 / (1.0 + (P / a) ** b)\n",
        "\n",
        "    def loglogN(P, a, b):      # normalized Q with Q0 fixed to 100\n",
        "        return 100.0 / (1.0 + (P / a) ** b)\n",
        "\n",
        "    # --- fit per-mouse (RAW) ---\n",
        "    raw_fit_rows = []\n",
        "    for mouse, sub in demand_mouse.groupby(\"Mouse_ID\"):\n",
        "        P = sub[\"PricePaid\"].to_numpy(dtype=float)\n",
        "        Q = sub[\"PelletCount\"].to_numpy(dtype=float)\n",
        "        nuniq = np.unique(P).size\n",
        "        if nuniq < 3:\n",
        "            print(f\"[{mouse}] demand fit skipped (raw): need â‰¥3 unique FR levels, have {nuniq}.\")\n",
        "            continue\n",
        "        try:\n",
        "            popt, _ = curve_fit(\n",
        "                loglog3, P, Q,\n",
        "                p0=[Q.max(), float(np.median(P)), 2.0],\n",
        "                bounds=([1e-6, 1e-6, 1e-3], [np.inf, np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            raw_fit_rows.append({\n",
        "                \"Mouse_ID\": mouse,\n",
        "                \"Demand_Q0_raw\": float(popt[0]),\n",
        "                \"Demand_alpha_raw\": float(popt[1]),\n",
        "                \"Demand_beta_raw\": float(popt[2]),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{mouse}] demand curve fit failed (raw): {e}\")\n",
        "\n",
        "    demand_fit_raw = pd.DataFrame(raw_fit_rows)\n",
        "\n",
        "    # --- fit per-mouse (Q-normalized, FR on x-axis) ---\n",
        "    fr_fit_rows = []\n",
        "    for mouse, sub in normalized.groupby(\"Mouse_ID\"):\n",
        "        P = sub[\"P_FR\"].to_numpy(dtype=float)    # FR requirement (unscaled)\n",
        "        Q = sub[\"Q_norm\"].to_numpy(dtype=float)  # normalized quantity\n",
        "\n",
        "        nuniq = np.unique(P).size\n",
        "        if nuniq < 3:\n",
        "            print(f\"[{mouse}] demand fit skipped (norm-Q): need â‰¥3 unique FR levels, have {nuniq}.\")\n",
        "            continue\n",
        "        try:\n",
        "            popt, _ = curve_fit(\n",
        "                loglogN, P, Q,\n",
        "                p0=[float(np.median(P)), 2.0],\n",
        "                bounds=([1e-6, 1e-3], [np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            fr_fit_rows.append({\n",
        "                \"Mouse_ID\": mouse,\n",
        "                \"Demand_alpha_FR\": float(popt[0]),  # alpha is now in FR units\n",
        "                \"Demand_beta_FR\": float(popt[1]),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[{mouse}] demand curve fit failed (norm-Q): {e}\")\n",
        "\n",
        "    demand_fit_fr = pd.DataFrame(fr_fit_rows)\n",
        "\n",
        "    # --- combine demand metrics per mouse ---\n",
        "    demand_metrics = demand_fit_raw.merge(demand_fit_fr, on=\"Mouse_ID\", how=\"outer\")\n",
        "\n",
        "# ---------- Attach Mouse_ID (primary) from metadata_df, then merge all metadata ----------\n",
        "# 1) Direct map filename -> Mouse_ID\n",
        "mouse_map = md.set_index('filename')['Mouse_ID']\n",
        "PRmetrics['Mouse_ID'] = PRmetrics['filename'].map(mouse_map)\n",
        "\n",
        "# Attach demand metrics now that PRmetrics has Mouse_ID\n",
        "PRmetrics = PRmetrics.merge(demand_metrics, on=\"Mouse_ID\", how=\"left\")\n",
        "\n",
        "# 2) Fallback: substring match on known Mouse_IDs if still missing\n",
        "if PRmetrics['Mouse_ID'].isna().any():\n",
        "    def _file_base_lower(pathlike):\n",
        "        return os.path.splitext(os.path.basename(str(pathlike)))[0].lower()\n",
        "\n",
        "    known_ids = md['Mouse_ID'].dropna().unique().tolist()\n",
        "    for i, row in PRmetrics.loc[PRmetrics['Mouse_ID'].isna()].iterrows():\n",
        "        base = _file_base_lower(row['filename'])\n",
        "        hits = [mid for mid in known_ids if str(mid).lower() in base]\n",
        "        if len(hits) == 1:\n",
        "            PRmetrics.at[i, 'Mouse_ID'] = hits[0]\n",
        "        elif len(hits) > 1:\n",
        "            longest = max(len(str(h)) for h in hits)\n",
        "            best = [h for h in hits if len(str(h)) == longest]\n",
        "            if len(best) == 1:\n",
        "                PRmetrics.at[i, 'Mouse_ID'] = best[0]\n",
        "\n",
        "# 3) Merge metadata: prefer Mouse_ID, fallback to filename if no Mouse_ID match\n",
        "md_mouse_unique = md.drop_duplicates(subset=['Mouse_ID'], keep='first')\n",
        "pm = PRmetrics.merge(md_mouse_unique, on='Mouse_ID', how='left', suffixes=('', '_md'))\n",
        "\n",
        "needs_fallback = pm['Mouse_ID'].isna() | pm.filter(regex='_md$').isna().all(axis=1)\n",
        "if needs_fallback.any():\n",
        "    md_file_unique = md.drop_duplicates(subset=['filename'], keep='first')\n",
        "    pm_fb = PRmetrics.loc[needs_fallback].merge(\n",
        "        md_file_unique, on='filename', how='left', suffixes=('', '_md')\n",
        "    )\n",
        "    PRmetrics_merged = pd.concat([pm.loc[~needs_fallback], pm_fb], ignore_index=True)\n",
        "else:\n",
        "    PRmetrics_merged = pm\n",
        "from IPython.display import display, HTML\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "bm = PRmetrics_merged.copy()\n",
        "\n",
        "# Ensure simple filename\n",
        "if \"filename\" not in bm.columns:\n",
        "    if \"File\" in bm.columns:\n",
        "        bm[\"filename\"] = bm[\"File\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    else:\n",
        "        raise RuntimeError(\"PRmetrics_merged must contain 'filename' (or 'File').\")\n",
        "\n",
        "# PR metrics to suffix (now INCLUDING demand parameters)\n",
        "metric_cols = [\n",
        "    # core PR\n",
        "    \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\", \"PokesPerPellet\",\n",
        "    \"MedianBreakPoint\", \"MaxBreakPoint\", \"Numberofblocks\", \"daily pellets\",\n",
        "    # optional timing if present\n",
        "    \"RetrievalTime\", \"InterPelletInterval\", \"PokeTime\",\n",
        "    # demand curve (raw + FR-normalized Q)\n",
        "    \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "    \"Demand_alpha_FR\", \"Demand_beta_FR\",\n",
        "]\n",
        "metric_cols = [c for c in metric_cols if c in bm.columns]\n",
        "\n",
        "# Session_type source: prefer column; else from feds[i].attrs\n",
        "if \"Session_type\" in bm.columns:\n",
        "    session_series = bm[\"Session_type\"].astype(str).str.strip()\n",
        "else:\n",
        "    def _basename(p): return os.path.basename(str(p))\n",
        "    sess_map = {\n",
        "        _basename(getattr(feds[i], \"name\", f\"File_{i}\")):\n",
        "        (feds[i].attrs.get(\"Session_type\") or \"Unknown\")\n",
        "        for i in range(len(feds))\n",
        "    }\n",
        "    session_series = bm[\"filename\"].map(sess_map).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "session_series = session_series.str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "bm[\"_Session_type_for_csv\"] = session_series\n",
        "\n",
        "def with_session_suffix_for_csv(df, metrics, session_col=\"_Session_type_for_csv\"):\n",
        "    df = df.copy()\n",
        "    if session_col not in df.columns or not metrics:\n",
        "        return df.drop(columns=[session_col], errors=\"ignore\")\n",
        "    present = [m for m in metrics if m in df.columns]\n",
        "    for m in present:\n",
        "        for sess in df[session_col].dropna().unique():\n",
        "            mask = df[session_col] == sess\n",
        "            col_name = f\"{m}_{sess}\"\n",
        "            if col_name not in df.columns:\n",
        "                df[col_name] = np.nan\n",
        "            df.loc[mask, col_name] = df.loc[mask, m]\n",
        "        df.drop(columns=[m], inplace=True, errors=\"ignore\")\n",
        "    return df.drop(columns=[session_col], errors=\"ignore\")\n",
        "\n",
        "PRmetrics_session_csv = with_session_suffix_for_csv(bm, metric_cols)\n",
        "\n",
        "# ---------- Save CSV ----------\n",
        "csv_name = f\"PRmetrics_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "PRmetrics_session_csv.to_csv(csv_name, index=False)\n",
        "display(HTML(f\"<b>âœ“ Saved PR metrics CSV to:</b> <code>{csv_name}</code>\"))\n",
        "\n",
        "# Optional download button (works in Colab)\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name)}</code>â€¦\"\n",
        "        gfiles.download(csv_name)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n"
      ],
      "metadata": {
        "id": "hrdyJUDo0QPh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot normalized demand curves\n",
        "try:\n",
        "    if isinstance(globals().get(\"demand_ui\"), dict):\n",
        "        for k in [\"slider\", \"save_btn\", \"status\", \"out\", \"box\"]:\n",
        "            w = demand_ui.get(k)\n",
        "            if hasattr(w, \"close\"):\n",
        "                w.close()\n",
        "        del demand_ui\n",
        "except Exception:\n",
        "    pass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "from ipywidgets import interact, IntSlider\n",
        "from IPython.display import clear_output\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.patheffects as pe\n",
        "from matplotlib.colors import LogNorm, Normalize\n",
        "\n",
        "\n",
        "# ---------- Build normalized_demand_df if missing ----------\n",
        "def _basename(p):\n",
        "    s = str(p).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "if \"normalized_demand_df\" not in globals() or not isinstance(globals().get(\"normalized_demand_df\"), pd.DataFrame) or globals()[\"normalized_demand_df\"].empty:\n",
        "    assert 'feds' in globals() and isinstance(feds, (list, tuple)) and len(feds) > 0, \\\n",
        "        \"Need `feds` (list of FED3 DataFrames) to derive normalized_demand_df.\"\n",
        "\n",
        "    file_names = [_basename(getattr(df, \"name\", f\"File_{i}\")) for i, df in enumerate(feds)]\n",
        "    file_to_df = dict(zip(file_names, feds))\n",
        "\n",
        "    rows = []\n",
        "    for fn, df in file_to_df.items():\n",
        "        if {\"Event\", \"Block_Pellet_Count\"} - set(df.columns):\n",
        "            continue\n",
        "        pellets = df[df[\"Event\"].astype(str) == \"Pellet\"].copy()\n",
        "        if pellets.empty:\n",
        "            continue\n",
        "        bpc = pd.to_numeric(pellets[\"Block_Pellet_Count\"], errors=\"coerce\")\n",
        "        counts = (\n",
        "            pellets.assign(Block_Pellet_Count=bpc)\n",
        "                   .dropna(subset=[\"Block_Pellet_Count\"])\n",
        "                   .groupby(\"Block_Pellet_Count\")\n",
        "                   .size()\n",
        "        )\n",
        "        for price, cnt in counts.items():\n",
        "            rows.append({\"File\": str(fn), \"PricePaid\": int(price), \"PelletCount\": int(cnt)})\n",
        "\n",
        "    normalized_demand_df = pd.DataFrame(rows)\n",
        "    if not normalized_demand_df.empty:\n",
        "        keep = normalized_demand_df.groupby(\"File\")[\"PricePaid\"].nunique()\n",
        "        normalized_demand_df = normalized_demand_df[\n",
        "            normalized_demand_df[\"File\"].isin(keep[keep >= 3].index)\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "# Preconditions\n",
        "if \"normalized_demand_df\" not in globals() or not isinstance(normalized_demand_df, pd.DataFrame) or normalized_demand_df.empty:\n",
        "    raise RuntimeError(\"normalized_demand_df is missing or empty. Expect columns: File, PricePaid, PelletCount.\")\n",
        "\n",
        "for col in [\"File\", \"PricePaid\", \"PelletCount\"]:\n",
        "    if col not in normalized_demand_df.columns:\n",
        "        raise RuntimeError(f\"normalized_demand_df must contain '{col}'\")\n",
        "\n",
        "# ---------- Prepare data (Q-only normalization) ----------\n",
        "data = normalized_demand_df.copy()\n",
        "data[\"File\"]        = data[\"File\"].astype(str)\n",
        "data[\"PricePaid\"]   = pd.to_numeric(data[\"PricePaid\"], errors=\"coerce\")\n",
        "data[\"PelletCount\"] = pd.to_numeric(data[\"PelletCount\"], errors=\"coerce\")\n",
        "data = data.dropna(subset=[\"PricePaid\", \"PelletCount\"])\n",
        "\n",
        "def _baseline_B(sub):\n",
        "    return sub.loc[sub[\"PricePaid\"].eq(sub[\"PricePaid\"].min()), \"PelletCount\"].mean()\n",
        "\n",
        "B_by_file = data.groupby(\"File\", as_index=True).apply(_baseline_B).rename(\"B\")\n",
        "q_by_file = (100.0 / B_by_file).rename(\"q\")\n",
        "\n",
        "data = data.merge(q_by_file, left_on=\"File\", right_index=True, how=\"left\")\n",
        "data[\"Q_norm\"] = data[\"PelletCount\"] * data[\"q\"]\n",
        "\n",
        "files = sorted(data[\"File\"].unique().tolist())\n",
        "\n",
        "# Optional mouse labels\n",
        "fname_to_mouse = {}\n",
        "if \"metadata_df\" in globals() and isinstance(metadata_df, pd.DataFrame):\n",
        "    md_tmp = metadata_df.copy()\n",
        "    if \"filename\" in md_tmp.columns and \"Mouse_ID\" in md_tmp.columns:\n",
        "        md_tmp[\"filename\"] = md_tmp[\"filename\"].astype(str).apply(lambda p: p.split(\"/\")[-1])\n",
        "        fname_to_mouse = md_tmp.set_index(\"filename\")[\"Mouse_ID\"].astype(str).to_dict()\n",
        "\n",
        "# ---------- Model ----------\n",
        "def log_logistic_norm(P, alpha, beta):\n",
        "    return 100.0 / (1.0 + (P / alpha)**beta)\n",
        "\n",
        "# ---------- Plotter (returns fig; does not call plt.show) ----------\n",
        "def plot_mouse(idx: int):\n",
        "    if len(files) == 0:\n",
        "        raise RuntimeError(\"No files in data.\")\n",
        "    if not (0 <= idx < len(files)):\n",
        "        raise RuntimeError(f\"Out-of-range index {idx} for {len(files)} files.\")\n",
        "\n",
        "    file = files[idx]\n",
        "    sub = data[data[\"File\"] == file].copy().sort_values(\"PricePaid\")\n",
        "\n",
        "    FR = sub[\"PricePaid\"].to_numpy(dtype=float)\n",
        "    Qn = sub[\"Q_norm\"].to_numpy(dtype=float)\n",
        "\n",
        "    # Fit in FR space\n",
        "    alpha_FR = beta_FR = np.nan\n",
        "    if np.isfinite(FR).any() and np.isfinite(Qn).any():\n",
        "        try:\n",
        "            alpha0_fr = float(np.nanmedian(FR))\n",
        "            popt_fr, _ = curve_fit(\n",
        "                log_logistic_norm, FR, Qn,\n",
        "                p0=[max(alpha0_fr, 1.0), 2.0],\n",
        "                bounds=([1e-6, 1e-3], [np.inf, 50.0]),\n",
        "                maxfev=20000\n",
        "            )\n",
        "            alpha_FR, beta_FR = [float(p) for p in popt_fr]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Figure\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "\n",
        "    # choose bounds from your finite, positive FR values\n",
        "    FR_pos = FR[np.isfinite(FR) & (FR > 0)]\n",
        "    vmin = float(np.nanmin(FR_pos)) if FR_pos.size else 1.0\n",
        "    vmax = float(np.nanmax(FR_pos)) if FR_pos.size else 100.0\n",
        "\n",
        "    # Log-normalize so equal ratios get equal color steps (matches log x-axis)\n",
        "    norm = LogNorm(vmin=vmin, vmax=vmax)\n",
        "\n",
        "\n",
        "    sc = ax.scatter(FR, Qn, c=FR, cmap=\"spring\", norm=norm, s=30, edgecolor=\"none\", alpha=0.9)\n",
        "\n",
        "\n",
        "    FR_pos = FR[np.isfinite(FR) & (FR > 0)]\n",
        "    xmin = float(np.nanmin(FR_pos)) if FR_pos.size else 1.0\n",
        "    xmin = max(1.0, xmin)\n",
        "    xmax_data = float(np.nanmax(FR_pos)) if FR_pos.size else 100.0\n",
        "    xmax = max(100.0, xmax_data)\n",
        "    if xmax <= xmin:\n",
        "        xmax = xmin * 1.5\n",
        "    ax.set_xlim(xmin, xmax)\n",
        "    ax.set_xscale(\"log\")\n",
        "\n",
        "    if np.isfinite(alpha_FR) and np.isfinite(beta_FR):\n",
        "        Pfit = np.logspace(np.log10(xmin), np.log10(xmax), 400)\n",
        "        Qfit = log_logistic_norm(Pfit, alpha_FR, beta_FR)\n",
        "        ax.plot(Pfit, Qfit, linewidth=2)\n",
        "\n",
        "        ax.vlines(alpha_FR, ymin=0, ymax=50, linestyle=\"--\", linewidth=1.5)\n",
        "        ax.scatter([alpha_FR], [50], zorder=5)\n",
        "        ax.annotate(\n",
        "            rf\"$\\alpha_{{FR}}$ = {alpha_FR:.3g}\",\n",
        "            xy=(alpha_FR, 50),\n",
        "            xytext=(8, 8), textcoords=\"offset points\",\n",
        "            fontsize=11, ha=\"left\", va=\"bottom\",\n",
        "            path_effects=[pe.withStroke(linewidth=2, foreground=\"white\", alpha=0.7)]\n",
        "        )\n",
        "\n",
        "        dQdP_alpha = -(100.0 * beta_FR) / (4.0 * alpha_FR)\n",
        "        dx = alpha_FR * 0.2\n",
        "        x1, x2 = alpha_FR - dx/2.0, alpha_FR + dx/2.0\n",
        "        y1 = 50 + dQdP_alpha * (x1 - alpha_FR)\n",
        "        y2 = 50 + dQdP_alpha * (x2 - alpha_FR)\n",
        "        ax.plot([x1, x2], [y1, y2], linewidth=4)\n",
        "        xm, ym = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
        "        ax.annotate(\n",
        "            rf\"$\\beta_{{FR}}$ = {beta_FR:.3g}\",\n",
        "            xy=(xm, ym), xytext=(8, 0), textcoords=\"offset points\",\n",
        "            fontsize=11, ha=\"left\", va=\"center\",\n",
        "            path_effects=[pe.withStroke(linewidth=2, foreground=\"white\", alpha=0.7)]\n",
        "        )\n",
        "\n",
        "    title = fname_to_mouse.get(file, None) or os.path.basename(str(file))\n",
        "    ax.set_title(str(title))\n",
        "    ax.set_xlabel(\"Food Price (FR)\")\n",
        "    ax.set_ylabel(\"Consumption\")\n",
        "    ax.set_yticks(np.arange(0, 101, 20))\n",
        "    ax.set_ylim(0, 105)\n",
        "\n",
        "    fr_ticks = np.array([1, 5, 10, 20, 30, 50, 75, 100], dtype=float)\n",
        "    fr_ticks = fr_ticks[(fr_ticks >= xmin) & (fr_ticks <= xmax)]\n",
        "    if fr_ticks.size:\n",
        "        ax.set_xticks(fr_ticks)\n",
        "        ax.set_xticklabels([str(int(fr)) for fr in fr_ticks])\n",
        "        for fr in fr_ticks:\n",
        "            ax.axvline(fr, alpha=0.12, lw=1, zorder=0)\n",
        "\n",
        "    ax.grid(False)\n",
        "    fig.tight_layout()\n",
        "    return fig, file\n",
        "\n",
        "# ---------- UI: single instance ----------\n",
        "demand_ui = {}\n",
        "\n",
        "def _sanitize_filename(s: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)\n",
        "\n",
        "slider = widgets.IntSlider(\n",
        "    min=0, max=max(0, len(files)-1), step=1, value=0,\n",
        "    description=\"File\", continuous_update=True\n",
        ")\n",
        "save_btn = widgets.Button(description=\"Save PDF\")\n",
        "status = widgets.HTML()\n",
        "out = widgets.Output()\n",
        "\n",
        "def _update_plot(change=None):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        fig, file = plot_mouse(slider.value)\n",
        "        display(fig)      # show exactly one figure\n",
        "        plt.close(fig)    # close backend handle to prevent accumulation\n",
        "        demand_ui[\"_last_fig\"] = fig\n",
        "        demand_ui[\"_last_file\"] = file\n",
        "        status.value = \"\" # clear old status\n",
        "\n",
        "def _save_pdf(_):\n",
        "    fig = demand_ui.get(\"_last_fig\")\n",
        "    file = demand_ui.get(\"_last_file\", \"figure\")\n",
        "    if fig is None:\n",
        "        status.value = \"<span style='color:#b00'>No figure to save.</span>\"\n",
        "        return\n",
        "    base = _sanitize_filename(os.path.splitext(os.path.basename(str(file)))[0])\n",
        "    fname = f\"demand_{base}_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "    # Save from the stored fig object; even though we closed the backend handle,\n",
        "    # the Figure instance retains its artists and can still be saved.\n",
        "    fig.savefig(fname, format=\"pdf\", bbox_inches=\"tight\")\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Preparing download: <code>{fname}</code>â€¦\"\n",
        "        gfiles.download(fname)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{os.path.abspath(fname)}</code>.\"\n",
        "\n",
        "slider.observe(_update_plot, names=\"value\")\n",
        "save_btn.on_click(_save_pdf)\n",
        "\n",
        "# Keep references so we can close them next run\n",
        "box = widgets.HBox([slider, save_btn])\n",
        "demand_ui.update({\"slider\": slider, \"save_btn\": save_btn, \"status\": status, \"out\": out, \"box\": box})\n",
        "\n",
        "# Initial render\n",
        "_update_plot()\n",
        "display(box)\n",
        "display(status)\n",
        "display(out)"
      ],
      "metadata": {
        "id": "70luc0dCbFaq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Group for plotting\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import tqdm\n",
        "\n",
        "assert 'metadata_df' in globals() and isinstance(metadata_df, pd.DataFrame), \\\n",
        "    \"metadata_df not found. Build/rematch Key_Df and set metadata_df = Key_Df.copy() first.\"\n",
        "\n",
        "def _build_file_column(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return df[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    if \"FED3_from_file\" in df.columns and \"Date_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str) + \"_\" + df[\"Date_from_file\"].astype(str)\n",
        "    if \"FED3_from_file\" in df.columns:\n",
        "        return \"FED\" + df[\"FED3_from_file\"].astype(str)\n",
        "    return df.index.astype(str)\n",
        "\n",
        "def _norm_val(x):\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
        "        return \"UNK\"\n",
        "    return s.upper()\n",
        "\n",
        "def _build_group_row(row, ordered_cols):\n",
        "    if not ordered_cols:\n",
        "        return \"ALL\"\n",
        "    return \" | \".join(_norm_val(row[c]) for c in ordered_cols)\n",
        "\n",
        "def build_mapping(ordered_cols):\n",
        "    _meta = metadata_df.copy()\n",
        "    _meta[\"filename\"] = _build_file_column(_meta)\n",
        "    _meta[\"Group\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols), axis=1)\n",
        "    mapping = (\n",
        "        _meta[[\"filename\", \"Group\"]]\n",
        "        .dropna(subset=[\"filename\"])\n",
        "        .drop_duplicates()\n",
        "        .sort_values([\"Group\", \"filename\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return mapping\n",
        "\n",
        "def _unique_keep_order(seq):\n",
        "    seen = set(); out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ---------- UI (fixed sizes + grid) ----------\n",
        "PX_W = \"260px\"   # list box width\n",
        "PX_H = \"160px\"   # list box height\n",
        "BTN_W = \"160px\"  # button column width\n",
        "HDR_H = \"28px\"   # header cell height (consistent across all headers)\n",
        "\n",
        "title = widgets.HTML(\"<h3>Select columns to group by for X and Hue, then reorder X to set hierarchy</h3>\")\n",
        "try:\n",
        "    EXCLUDE_LOWER  # if someone defined it earlier, keep it\n",
        "except NameError:\n",
        "    EXCLUDE_LOWER = set()  # no exclusions\n",
        "all_cols = sorted((c for c in metadata_df.columns if str(c).lower() not in EXCLUDE_LOWER), key=str.lower)\n",
        "\n",
        "def header(text):\n",
        "    # Normalize header height/margins so they align perfectly in the grid row\n",
        "    return widgets.HTML(\n",
        "        f\"<div style='height:{HDR_H};display:flex;align-items:flex-end;'>\"\n",
        "        f\"<h4 style=\\\"margin:0;\\\">{text}</h4></div>\"\n",
        "    )\n",
        "\n",
        "# Headers (row 1 of grid)\n",
        "available_hdr = header(\"Available\")\n",
        "actions_hdr   = header(\"Actions\")\n",
        "x_hdr         = header(\"X grouping\")\n",
        "hue_hdr       = header(\"Hue grouping\")\n",
        "\n",
        "# Widgets (row 2 of grid)\n",
        "available = widgets.SelectMultiple(\n",
        "    options=all_cols, value=tuple(), rows=14,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_x = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "right_hue = widgets.Select(\n",
        "    options=[], value=None, rows=8,\n",
        "    layout=widgets.Layout(\n",
        "        width=PX_W, height=PX_H, min_width=PX_W, max_width=PX_W,\n",
        "        min_height=PX_H, max_height=PX_H, flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Buttons\n",
        "btn_add_x    = widgets.Button(description=\"Add to X â–¶\", button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_add_hue  = widgets.Button(description=\"Add to Hue â–¶\",button_style='primary', layout=widgets.Layout(width=BTN_W))\n",
        "btn_clear    = widgets.Button(description=\"Clear\", button_style='danger', layout=widgets.Layout(width=BTN_W))\n",
        "btn_up       = widgets.Button(description=\"â†‘ Up (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_down     = widgets.Button(description=\"â†“ Down (X only)\", layout=widgets.Layout(width=BTN_W))\n",
        "btn_build    = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "controls_col = widgets.VBox(\n",
        "    [btn_add_x, btn_add_hue, btn_clear, btn_up, btn_down],\n",
        "    layout=widgets.Layout(\n",
        "        align_items=\"center\",\n",
        "        width=BTN_W, min_width=BTN_W, max_width=BTN_W,\n",
        "        height=PX_H, min_height=PX_H, max_height=PX_H,\n",
        "        flex=\"0 0 auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "btn_build = widgets.Button(description=\"Build Groups\", button_style='success', layout=widgets.Layout(width=\"160px\"))\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_add_x(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_x.options) + sel)\n",
        "    right_x.value = None\n",
        "    right_x.options = new_opts\n",
        "    right_x.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_add_hue(_):\n",
        "    sel = list(available.value)\n",
        "    if not sel: return\n",
        "    new_opts = _unique_keep_order(list(right_hue.options) + sel)\n",
        "    right_hue.value = None\n",
        "    right_hue.options = new_opts\n",
        "    right_hue.value = new_opts[-1] if new_opts else None\n",
        "\n",
        "def on_clear(_):\n",
        "    right_x.value = None; right_x.options = []\n",
        "    right_hue.value = None; right_hue.options = []\n",
        "\n",
        "def on_up(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i > 0:\n",
        "        opts[i-1], opts[i] = opts[i], opts[i-1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_down(_):\n",
        "    item = right_x.value\n",
        "    if item is None: return\n",
        "    opts = list(right_x.options)\n",
        "    i = opts.index(item)\n",
        "    if i < len(opts) - 1:\n",
        "        opts[i+1], opts[i] = opts[i], opts[i+1]\n",
        "        right_x.value = None; right_x.options = opts; right_x.value = item\n",
        "\n",
        "def on_build(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ordered_cols_x = list(right_x.options)\n",
        "        ordered_cols_hue = list(right_hue.options)\n",
        "\n",
        "        mapping_x = build_mapping(ordered_cols_x)\n",
        "        mapping_hue = build_mapping(ordered_cols_hue)\n",
        "\n",
        "        _meta = metadata_df.copy()\n",
        "        _meta[\"filename\"] = _build_file_column(_meta)\n",
        "        _meta[\"XGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_x), axis=1)\n",
        "        _meta[\"HueGroup\"] = _meta.apply(lambda r: _build_group_row(r, ordered_cols_hue), axis=1)\n",
        "        mapping_both = (\n",
        "            _meta[[\"filename\", \"XGroup\", \"HueGroup\"]]\n",
        "            .dropna(subset=[\"filename\"])\n",
        "            .drop_duplicates()\n",
        "            .sort_values([\"XGroup\", \"HueGroup\", \"filename\"])\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        globals()['files_to_group_x'] = mapping_x.copy()\n",
        "        globals()['files_to_group_hue'] = mapping_hue.copy()\n",
        "        globals()['files_to_group_both'] = mapping_both.copy()\n",
        "        globals()['selected_group_cols_x'] = ordered_cols_x.copy()\n",
        "        globals()['selected_group_cols_hue'] = ordered_cols_hue.copy()\n",
        "\n",
        "        print(\"X-axis grouping (hierarchy):\", ordered_cols_x if ordered_cols_x else [\"ALL\"])\n",
        "        print(f\"Total unique files (X map): {mapping_x['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>X-group summary</b>\"))\n",
        "        display((mapping_x.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "\n",
        "        print(\"\\nHue grouping:\", ordered_cols_hue if ordered_cols_hue else [\"ALL\"])\n",
        "        print(f\"Total unique files (Hue map): {mapping_hue['filename'].nunique()}\")\n",
        "        display(widgets.HTML(\"<b>Hue-group summary</b>\"))\n",
        "        display((mapping_hue.groupby(\"Group\", dropna=False)[\"filename\"]\n",
        "                 .nunique().sort_values(ascending=False)\n",
        "                 .rename(\"UniqueFiles\").to_frame()))\n",
        "        print(\"\\nCombined mapping available as `files_to_group_both` (filename, XGroup, HueGroup)\")\n",
        "\n",
        "# Wire up\n",
        "btn_add_x.on_click(on_add_x)\n",
        "btn_add_hue.on_click(on_add_hue)\n",
        "btn_clear.on_click(on_clear)\n",
        "btn_up.on_click(on_up)\n",
        "btn_down.on_click(on_down)\n",
        "btn_build.on_click(on_build)\n",
        "\n",
        "# ----- Grid layout -----\n",
        "grid = widgets.GridBox(\n",
        "    children=[\n",
        "        available_hdr, actions_hdr, x_hdr, hue_hdr,     # row 1: headers\n",
        "        available,     controls_col, right_x, right_hue # row 2: widgets\n",
        "    ],\n",
        "    layout=widgets.Layout(\n",
        "        grid_template_columns=f\"{PX_W} {BTN_W} {PX_W} {PX_W}\",\n",
        "        grid_template_rows=\"auto auto\",\n",
        "        grid_gap=\"6px 16px\",\n",
        "        align_items=\"flex-start\",\n",
        "        justify_items=\"flex-start\",\n",
        "        width=\"100%\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([title, grid, widgets.HBox([btn_build]), output])\n",
        "display(ui)\n"
      ],
      "metadata": {
        "id": "DNxvpj9ZrC8b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Metrics!\n",
        "\n",
        "import os, time, shutil, re, itertools\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Optional stats libs\n",
        "try:\n",
        "    import pingouin as pg\n",
        "except Exception:\n",
        "    pg = None\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    from statsmodels.formula.api import ols\n",
        "except Exception:\n",
        "    sm, ols = None, None\n",
        "\n",
        "# Optional Colab download\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "except Exception:\n",
        "    colab_files = None\n",
        "\n",
        "ALPHA = 0.6  # bar & dot alpha\n",
        "\n",
        "# -----------------------\n",
        "# 0) Preconditions & base table\n",
        "# -----------------------\n",
        "if \"PRmetrics\" not in globals() or not isinstance(PRmetrics, pd.DataFrame) or PRmetrics.empty:\n",
        "    raise RuntimeError(\"PRmetrics not found/empty. Run the PR metrics cell first.\")\n",
        "\n",
        "if \"files_to_group_both\" not in globals() or not isinstance(files_to_group_both, pd.DataFrame) or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"files_to_group_both not found/empty. Run the 'Group for plotting' cell and Build Groups.\")\n",
        "\n",
        "bm = PRmetrics.copy()\n",
        "if \"File\" not in bm.columns:\n",
        "    if \"filename\" in bm.columns:\n",
        "        bm[\"File\"] = bm[\"filename\"].apply(lambda p: os.path.basename(str(p)))\n",
        "    else:\n",
        "        raise RuntimeError(\"PRmetrics must contain 'File' or 'filename'.\")\n",
        "\n",
        "# -----------------------\n",
        "# -----------------------\n",
        "# Merge in XGroup/HueGroup from grouping widget\n",
        "# -----------------------\n",
        "def _basename_col(s):\n",
        "    return os.path.basename(str(s))\n",
        "\n",
        "def _src_name(df):\n",
        "    if \"filename\" in df.columns:\n",
        "        return \"filename\"\n",
        "    return None\n",
        "\n",
        "if (\"XGroup\" not in bm.columns) or (\"HueGroup\" not in bm.columns):\n",
        "    if 'files_to_group_both' in globals() and files_to_group_both is not None and not files_to_group_both.empty:\n",
        "        m = files_to_group_both.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"XGroup\",\"HueGroup\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"XGroup\"]   = bm[\"XGroup\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"HueGroup\"] = bm[\"HueGroup\"].fillna(\"UNASSIGNED\")\n",
        "    elif 'files_to_group' in globals() and files_to_group is not None and not files_to_group.empty:\n",
        "        m = files_to_group.copy()\n",
        "        m_src = _src_name(m)\n",
        "        if m_src is None:\n",
        "            raise RuntimeError(\"Grouping table must include 'filename' (or legacy 'File').\")\n",
        "        m[\"file_base\"]  = m[m_src].apply(_basename_col)\n",
        "        bm[\"file_base\"] = bm[\"filename\"].apply(_basename_col)\n",
        "        bm = bm.merge(m[[\"file_base\",\"Group\"]], on=\"file_base\", how=\"left\").drop(columns=[\"file_base\"])\n",
        "        bm[\"Group\"] = m[\"Group\"].fillna(\"UNASSIGNED\")\n",
        "        bm[\"XGroup\"] = bm[\"Group\"]\n",
        "        bm[\"HueGroup\"] = \"ALL\"\n",
        "    else:\n",
        "        raise RuntimeError(\"Missing X/Hue mapping. Run the grouping widget (Build Groups) first.\")\n",
        "def export_group_colors_to_global():\n",
        "    # Publish widget colors to a plain dict for other cells\n",
        "    globals()[\"group_colors\"] = {\n",
        "        g: (x_colors[g].value.strip() or \"tab:blue\")\n",
        "        for g in x_colors\n",
        "    }\n",
        "\n",
        "# -----------------------\n",
        "# 1) Melt to long format (numeric PR1 + fit params)\n",
        "# -----------------------\n",
        "base_metric_names = [\n",
        "    \"Left_Poke\", \"Right_Poke\", \"Total_Pokes\", \"Accuracy\", \"PokesPerPellet\",\n",
        "    \"MedianBreakPoint\", \"Numberofblocks\", \"daily pellets\",\n",
        "    \"Demand_Q0_raw\", \"Demand_alpha_raw\", \"Demand_beta_raw\",\n",
        "    \"Demand_alpha_FR\",\"Demand_beta_FR\",\n",
        "]\n",
        "\n",
        "metric_cols = []\n",
        "for c in bm.columns:\n",
        "    if pd.api.types.is_numeric_dtype(bm[c]):\n",
        "        for base in base_metric_names:\n",
        "            if c == base or c.startswith(base + \"_\"):\n",
        "                metric_cols.append(c); break\n",
        "seen = set(); metric_cols = [c for c in metric_cols if not (c in seen or seen.add(c))]\n",
        "if not metric_cols:\n",
        "    raise RuntimeError(\"No numeric metric columns found among expected Bandit metrics.\")\n",
        "\n",
        "candidate_id_vars = [\"Genotype\",\"Sex\",\"Strain\",\"Start_Date\",\"filename\",\"Mouse_ID\",\"Session_type\",\"XGroup\",\"HueGroup\"]\n",
        "id_vars = [c for c in candidate_id_vars if c in bm.columns]\n",
        "for need in [\"XGroup\",\"HueGroup\",\"filename\"]:\n",
        "    if need not in id_vars: id_vars.append(need)\n",
        "\n",
        "long_df = pd.melt(\n",
        "    bm,\n",
        "    id_vars=id_vars,\n",
        "    value_vars=metric_cols,\n",
        "    var_name=\"variable\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 2) Ordering helpers (hierarchical)\n",
        "# -----------------------\n",
        "def _is_wt_group(g):\n",
        "    u = str(g).strip().upper()\n",
        "    tokens = [t for t in re.split(r'[^A-Z0-9]+', u) if t]\n",
        "    return ('WT' in tokens) or ('WILDTYPE' in tokens)\n",
        "\n",
        "def _is_unassigned_token(s):\n",
        "    return (str(s).strip().upper() in {\"\", \"UNASSIGNED\", \"NONE\", \"NA\", \"N/A\"})\n",
        "\n",
        "def _x_levels(xname):\n",
        "    \"\"\"Split an XGroup label into hierarchical levels based on ' | ' and align to selected_group_cols_x if present.\"\"\"\n",
        "    s = str(xname)\n",
        "    parts = [p.strip() for p in s.split(\"|\")]\n",
        "    wanted = globals().get(\"selected_group_cols_x\", None)\n",
        "    if isinstance(wanted, (list, tuple)) and len(wanted) > 0:\n",
        "        if len(parts) < len(wanted):\n",
        "            parts = parts + [\"\"] * (len(wanted) - len(parts))\n",
        "        elif len(parts) > len(wanted):\n",
        "            parts = parts[:len(wanted)]\n",
        "    return parts\n",
        "\n",
        "def _hier_sort_key(g):\n",
        "    \"\"\"Sort by level1, then level2, ...; UNASSIGNED to the end; WT before others at deepest level.\"\"\"\n",
        "    lv = _x_levels(g)\n",
        "    norm = []\n",
        "    for tok in lv:\n",
        "        is_blank = 1 if _is_unassigned_token(tok) else 0\n",
        "        norm.append((is_blank, str(tok).upper()))\n",
        "    deepest = lv[-1] if lv else \"\"\n",
        "    wt_rank = 0 if _is_wt_group(deepest) else 1  # remove if you want purely lexical\n",
        "    return tuple(norm) + (wt_rank, str(g).upper())\n",
        "\n",
        "def _order_x_groups(groups):\n",
        "    return sorted(groups, key=_hier_sort_key)\n",
        "\n",
        "def _choose_ref_group(order):\n",
        "    for g in order:\n",
        "        if _is_wt_group(g):\n",
        "            return g\n",
        "    return order[0] if order else None\n",
        "\n",
        "all_x = [g for g in sorted(long_df[\"XGroup\"].dropna().unique().tolist()) if g != \"UNASSIGNED\"] or [\"UNASSIGNED\"]\n",
        "ordered_x = _order_x_groups(all_x)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Controls (left column: groups & colors)\n",
        "# -----------------------\n",
        "named_defaults = [\n",
        "    \"dodgerblue\", \"red\", \"green\", \"orange\", \"purple\",\n",
        "    \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
        "\n",
        "x_checks, x_colors = {}, {}\n",
        "group_rows = []\n",
        "for i, g in enumerate(ordered_x):\n",
        "    chk = widgets.Checkbox(value=True, description=g, indent=False, layout=widgets.Layout(width=\"260px\"))\n",
        "    col = widgets.Text(value=named_defaults[i % len(named_defaults)],\n",
        "                       layout=widgets.Layout(width=\"120px\"))\n",
        "    x_checks[g] = chk\n",
        "    x_colors[g] = col\n",
        "    # more compact row\n",
        "    group_rows.append(widgets.HBox([chk, widgets.Label(\"\"), col],\n",
        "                                   layout=widgets.Layout(align_items=\"center\", height=\"28px\")))\n",
        "\n",
        "picker = widgets.VBox(group_rows, layout=widgets.Layout(gap=\"2px\"))\n",
        "\n",
        "btn_all  = widgets.Button(description=\"Select all\", layout=widgets.Layout(width=\"140px\"))\n",
        "btn_none = widgets.Button(description=\"Clear\", layout=widgets.Layout(width=\"140px\"))\n",
        "def _set_all(val):\n",
        "    for c in x_checks.values(): c.value = val\n",
        "btn_all.on_click(lambda _: _set_all(True))\n",
        "btn_none.on_click(lambda _: _set_all(False))\n",
        "\n",
        "# scrollable container for the (possibly long) group list\n",
        "picker_container = widgets.Box([picker],\n",
        "    layout=widgets.Layout(overflow=\"auto\", max_height=\"420px\",\n",
        "                          border=\"1px solid #ddd\", padding=\"6px\", width=\"360px\"))\n",
        "\n",
        "left_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Groups & Colors</b>\"),\n",
        "    widgets.HBox([btn_all, btn_none], layout=widgets.Layout(gap=\"8px\")),\n",
        "    picker_container\n",
        "], layout=widgets.Layout(width=\"380px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 4) Comparison controls (right column)\n",
        "# -----------------------\n",
        "mode_radio = widgets.ToggleButtons(\n",
        "    options=[(\"Reference group\", \"ref\"), (\"Select Pairs\", \"pairs\")],\n",
        "    value=\"ref\", description=\"\", style={\"button_width\":\"150px\"},\n",
        "    layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "ref_dropdown = widgets.Dropdown(\n",
        "    options=ordered_x, value=_choose_ref_group(ordered_x),\n",
        "    description=\"Reference:\", layout=widgets.Layout(width=\"320px\")\n",
        ")\n",
        "\n",
        "def _pair_label(a,b): return f\"{a} âŸ· {b}\"\n",
        "def _pair_value(a,b): return (a,b) if a <= b else (b,a)\n",
        "\n",
        "pairs_select = widgets.SelectMultiple(\n",
        "    options=[], value=[], description=\"Pairs\",\n",
        "    layout=widgets.Layout(width=\"360px\", height=\"320px\")\n",
        ")\n",
        "\n",
        "def _selected_x():\n",
        "    return _order_x_groups([g for g, cb in x_checks.items() if cb.value])\n",
        "\n",
        "def _pair_sort_key(a, b):\n",
        "    \"\"\"Rank pairs by level of first difference: later differences (within-group) rank first.\"\"\"\n",
        "    A = _x_levels(a); B = _x_levels(b)\n",
        "    L = max(len(A), len(B))\n",
        "    if len(A) < L: A += [\"\"] * (L - len(A))\n",
        "    if len(B) < L: B += [\"\"] * (L - len(B))\n",
        "    first_diff = next((i for i, (x, y) in enumerate(zip(A, B)) if x != y), L)\n",
        "    prefix = tuple(A[:first_diff])\n",
        "    return (-first_diff, prefix, tuple(A), tuple(B))\n",
        "\n",
        "def _update_ref_and_pairs(*_):\n",
        "    sel = _selected_x()\n",
        "    ref_dropdown.options = sel or [\"â€”\"]\n",
        "    if sel:\n",
        "        if ref_dropdown.value not in sel:\n",
        "            ref_dropdown.value = _choose_ref_group(sel)\n",
        "    else:\n",
        "        ref_dropdown.value = None\n",
        "\n",
        "    opts = []\n",
        "    for a, b in itertools.combinations(sel, 2):\n",
        "        lbl = _pair_label(a, b)\n",
        "        val = _pair_value(a, b)\n",
        "        opts.append((lbl, val))\n",
        "    opts.sort(key=lambda kv: _pair_sort_key(*kv[1]))\n",
        "    pairs_select.options = opts\n",
        "\n",
        "for cb in x_checks.values():\n",
        "    cb.observe(_update_ref_and_pairs, names=\"value\")\n",
        "_update_ref_and_pairs()\n",
        "\n",
        "# action buttons\n",
        "plot_btn = widgets.Button(description=\"Plot\", button_style=\"primary\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "save_btn = widgets.Button(description=\"Save Plots\", button_style=\"success\",\n",
        "                          layout=widgets.Layout(width=\"160px\"))\n",
        "\n",
        "# Right column layout\n",
        "right_col = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Statistical comparisons</b>\"),\n",
        "    mode_radio,\n",
        "    ref_dropdown,\n",
        "    pairs_select,\n",
        "    widgets.HBox([plot_btn, save_btn], layout=widgets.Layout(gap=\"8px\"))\n",
        "], layout=widgets.Layout(width=\"360px\"))\n",
        "\n",
        "# -----------------------\n",
        "# 5) Labels for stats/legend\n",
        "# -----------------------\n",
        "def _grouping_label(which=\"X\"):\n",
        "    if which.lower().startswith(\"x\"):\n",
        "        cols = globals().get(\"selected_group_cols_x\", [])\n",
        "        default = \"XGroup\"\n",
        "    else:\n",
        "        cols = globals().get(\"selected_group_cols_hue\", [])\n",
        "        default = \"HueGroup\"\n",
        "    cols = [str(c).strip() for c in (cols or []) if str(c).strip()]\n",
        "    return \" | \".join(cols) if cols else default\n",
        "\n",
        "# -----------------------\n",
        "# 6) Stats helpers (ANOVA with Hue)\n",
        "# -----------------------\n",
        "def _fmt_p(p):\n",
        "    if not np.isfinite(p): return \"n/a\"\n",
        "    return f\"p = {p:.3f}\" if p >= 0.001 else \"p < 0.001\"\n",
        "\n",
        "def _anova_subset(df):\n",
        "    \"\"\"\n",
        "    If >=2 Hue levels: two-way ANOVA (XGroup, HueGroup, interaction)\n",
        "    Else: one-way ANOVA (XGroup).\n",
        "    \"\"\"\n",
        "    out = {\"p_x\": np.nan, \"p_h\": np.nan, \"p_int\": np.nan, \"n_h\": 0, \"ok\": False, \"err\": None}\n",
        "    d = df.dropna(subset=[\"value\",\"XGroup\"])\n",
        "    if d.empty or d[\"XGroup\"].nunique() < 2:\n",
        "        out[\"err\"] = \"Too few groups\"; return out\n",
        "    n_h = d[\"HueGroup\"].nunique(dropna=True); out[\"n_h\"] = n_h\n",
        "    try:\n",
        "        if n_h >= 2:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=d).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            out[\"p_x\"]   = float(an.loc['C(XGroup)','PR(>F)'])\n",
        "            out[\"p_h\"]   = float(an.loc['C(HueGroup)','PR(>F)'])\n",
        "            out[\"p_int\"] = float(an.loc['C(XGroup):C(HueGroup)','PR(>F)'])\n",
        "            out[\"ok\"] = True\n",
        "        else:\n",
        "            model = ols('value ~ C(XGroup)', data=d).fit()\n",
        "            out[\"p_x\"] = float(model.f_pvalue); out[\"ok\"] = True\n",
        "    except Exception as e:\n",
        "        out[\"err\"] = str(e)\n",
        "    return out\n",
        "\n",
        "def _stats_text(dfm, x_label, hue_label, *, mode=\"ref\", ref_group=None, pair_list=None):\n",
        "    df = dfm.dropna(subset=[\"value\"]).copy()\n",
        "    g_n = df[\"XGroup\"].nunique(dropna=True)\n",
        "    h_n = df[\"HueGroup\"].nunique(dropna=True)\n",
        "\n",
        "    if mode == \"pairs\" and pair_list:\n",
        "        lines = [\"Selected pairwise ANOVA tests:\"]\n",
        "        for a,b in pair_list:\n",
        "            sub = df[df[\"XGroup\"].isin([a,b])]\n",
        "            res = _anova_subset(sub)\n",
        "            if not res[\"ok\"]:\n",
        "                lines.append(f\"{a} vs {b}: {res['err'] or 'failed'}\"); continue\n",
        "            if res[\"n_h\"] >= 2:\n",
        "                lines.append(\n",
        "                    f\"{a} vs {b} (Two-way: {x_label}, {hue_label})  \"\n",
        "                    f\"{x_label}: {_fmt_p(res['p_x'])} | {hue_label}: {_fmt_p(res['p_h'])} | \"\n",
        "                    f\"{x_label}Ã—{hue_label}: {_fmt_p(res['p_int'])}\"\n",
        "                )\n",
        "            else:\n",
        "                lines.append(f\"{a} vs {b} (One-way {x_label}): {_fmt_p(res['p_x'])}\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def fmt(p): return _fmt_p(p)\n",
        "    if g_n == 2 and h_n <= 1:\n",
        "        g1, g2 = sorted(df[\"XGroup\"].unique())\n",
        "        v1 = df[df[\"XGroup\"] == g1][\"value\"].dropna()\n",
        "        v2 = df[df[\"XGroup\"] == g2][\"value\"].dropna()\n",
        "        if len(v1) > 1 and len(v2) > 1:\n",
        "            p = pg.ttest(v1, v2, paired=False)[\"p-val\"].values[0]\n",
        "            return f\"t-test ({x_label}): {fmt(p)}\\n{g1} vs {g2}\"\n",
        "        return \"t-test: not enough data\"\n",
        "\n",
        "    if g_n >= 2 and h_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup) + C(HueGroup) + C(XGroup):C(HueGroup)', data=df).fit()\n",
        "            an = sm.stats.anova_lm(model, typ=2)\n",
        "            return (\n",
        "                \"Two-way ANOVA\\n\"\n",
        "                f\"{x_label}: {fmt(float(an.loc['C(XGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{hue_label}: {fmt(float(an.loc['C(HueGroup)','PR(>F)']))}\\n\"\n",
        "                f\"{x_label}Ã—{hue_label}: {fmt(float(an.loc['C(XGroup):C(HueGroup)','PR(>F)']))}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"ANOVA failed: {e}\"\n",
        "\n",
        "    if g_n >= 2:\n",
        "        try:\n",
        "            model = ols('value ~ C(XGroup)', data=df).fit()\n",
        "            return f\"One-way ANOVA ({x_label}): {fmt(float(model.f_pvalue))}\"\n",
        "        except Exception as e:\n",
        "            return f\"One-way ANOVA failed: {e}\"\n",
        "    return \"Too few groups for stats\"\n",
        "\n",
        "# -----------------------\n",
        "# 7) Plotting helpers\n",
        "# -----------------------\n",
        "def _p_to_stars(p):\n",
        "    if not np.isfinite(p): return \"\"\n",
        "    if p < 1e-4: return \"****\"\n",
        "    if p < 1e-3: return \"***\"\n",
        "    if p < 1e-2: return \"**\"\n",
        "    if p < 5e-2: return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def _dot_palette(hues):\n",
        "    hues = list(hues)\n",
        "    if len(hues) == 0: return {}\n",
        "    if len(hues) == 1: return {hues[0]: \"black\"}\n",
        "    if len(hues) == 2: return {hues[0]: \"white\", hues[1]: \"black\"}\n",
        "    defaults = plt.rcParams.get('axes.prop_cycle', None)\n",
        "    colors = defaults.by_key()['color'] if defaults else [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\"]\n",
        "    return {h: colors[i % len(colors)] for i, h in enumerate(hues)}\n",
        "\n",
        "def _draw_bracket(ax, x1, x2, y, h, text):\n",
        "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=\"black\", zorder=5)\n",
        "    ax.text((x1+x2)/2, y+h, text, ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "def _plot_metric_clean(df_metric, variable, x_color_map, *, mode=\"ref\", ref_group=None, pair_list=None, return_fig=False):\n",
        "    dfm = df_metric.copy()\n",
        "    order = _order_x_groups(dfm[\"XGroup\"].dropna().unique().tolist())\n",
        "    if not order: return None\n",
        "    if (not ref_group) or (ref_group not in order):\n",
        "        ref_group = _choose_ref_group(order)\n",
        "\n",
        "    x_label_name   = _grouping_label(\"X\")\n",
        "    hue_label_name = _grouping_label(\"Hue\")\n",
        "\n",
        "    hue_levels = [h for h in dfm[\"HueGroup\"].dropna().unique().tolist()]\n",
        "    pal_dots = _dot_palette(hue_levels)\n",
        "\n",
        "    width = max(2, 1 * len(order))\n",
        "    height = 4.0\n",
        "    fig, (ax_plot, ax_text) = plt.subplots(\n",
        "        1, 2, figsize=(width/1.2, height), gridspec_kw={'width_ratios': [3, 1]}\n",
        "    )\n",
        "\n",
        "    # Bars\n",
        "    bar_palette = [x_color_map.get(g, \"tab:blue\") for g in order]\n",
        "    sns.barplot(data=dfm, x=\"XGroup\", y=\"value\", order=order, ci=None, alpha=ALPHA, ax=ax_plot, palette=bar_palette)\n",
        "\n",
        "    # Points\n",
        "    sns.stripplot(\n",
        "        data=dfm, x=\"XGroup\", y=\"value\", order=order, hue=\"HueGroup\",\n",
        "        jitter=True, dodge=False, size=7, edgecolor=\"black\", linewidth=1,\n",
        "        palette=pal_dots, ax=ax_plot, zorder=3, alpha=ALPHA\n",
        "    )\n",
        "    if ax_plot.legend_ is not None:\n",
        "        ax_plot.legend_.remove()\n",
        "\n",
        "    # Annotations\n",
        "    y_min, y_max = ax_plot.get_ylim()\n",
        "    span = (y_max - y_min) if y_max > y_min else 1.0\n",
        "    bump = 0.06 * span\n",
        "    data_max = dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max\n",
        "\n",
        "    if mode == \"ref\" and (ref_group in order):\n",
        "        ref_vals = dfm[dfm[\"XGroup\"] == ref_group][\"value\"].dropna().to_numpy()\n",
        "        for g in order:\n",
        "            if g == ref_group: continue\n",
        "            vals = dfm[dfm[\"XGroup\"] == g][\"value\"].dropna().to_numpy()\n",
        "            if len(vals) >= 2 and len(ref_vals) >= 2:\n",
        "                try:\n",
        "                    p = float(pg.ttest(vals, ref_vals, paired=False)[\"p-val\"].values[0])\n",
        "                except Exception:\n",
        "                    p = np.nan\n",
        "                if np.isfinite(p) and p < 0.05:\n",
        "                    xloc = order.index(g)\n",
        "                    gmax = dfm[dfm[\"XGroup\"] == g][\"value\"].max()\n",
        "                    y_star = (gmax if np.isfinite(gmax) else data_max) + bump\n",
        "                    ax_plot.text(xloc, y_star, _p_to_stars(p),\n",
        "                                 ha=\"center\", va=\"bottom\", fontsize=16, fontweight=\"bold\")\n",
        "                    y_max = max(y_max, y_star + bump)\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    elif mode == \"pairs\" and pair_list:\n",
        "        base = (dfm[\"value\"].max() if dfm[\"value\"].notna().any() else y_max) + bump\n",
        "        step = 0.12 * span\n",
        "        k = 0\n",
        "        for a,b in pair_list:\n",
        "            if (a not in order) or (b not in order):\n",
        "                continue\n",
        "            sub = dfm[dfm[\"XGroup\"].isin([a,b])].dropna(subset=[\"value\"])\n",
        "            if sub[\"XGroup\"].nunique() < 2:\n",
        "                continue\n",
        "            res = _anova_subset(sub)\n",
        "            # draw bracket ONLY if XGroup effect significant\n",
        "            if res[\"ok\"] and np.isfinite(res[\"p_x\"]) and (res[\"p_x\"] < 0.05):\n",
        "                x1 = order.index(a); x2 = order.index(b)\n",
        "                if x1 > x2: x1, x2 = x2, x1\n",
        "                y_here = base + k * step\n",
        "                _draw_bracket(ax_plot, x1, x2, y_here, 0.04 * span, _p_to_stars(res[\"p_x\"]))\n",
        "                y_max = max(y_max, y_here + 0.08 * span)\n",
        "                k += 1\n",
        "        ax_plot.set_ylim(y_min, y_max)\n",
        "\n",
        "    ax_plot.set_title(\"\")\n",
        "    ax_plot.set_xlabel(\"\")\n",
        "    ax_plot.set_ylabel(variable)\n",
        "    plt.setp(ax_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    sns.despine(ax=ax_plot)\n",
        "\n",
        "    # Right panel: stats + Hue legend\n",
        "    ax_text.axis(\"off\")\n",
        "    ax_text.text(\n",
        "        0, 1,\n",
        "        _stats_text(dfm, x_label_name, hue_label_name, mode=mode, ref_group=ref_group, pair_list=pair_list),\n",
        "        va=\"top\", ha=\"left\", fontsize=12, transform=ax_text.transAxes\n",
        "    )\n",
        "    if len(hue_levels) >= 2:\n",
        "        handles = [plt.Line2D([0],[0], marker='o', linestyle='None',\n",
        "                              markerfacecolor=pal_dots[h], markeredgecolor='black', label=str(h))\n",
        "                   for h in hue_levels]\n",
        "        ax_text.legend(handles=handles, title=hue_label_name, loc=\"upper left\", bbox_to_anchor=(0, 0.6))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig if return_fig else plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# 8) Actions\n",
        "# -----------------------\n",
        "out = widgets.Output()\n",
        "\n",
        "def _selected_x_and_colors():\n",
        "    sel = _selected_x()\n",
        "    color_map = {}\n",
        "    for g in sel:\n",
        "        val = x_colors[g].value.strip()\n",
        "        color_map[g] = val if val else \"tab:blue\"\n",
        "    return sel, color_map\n",
        "\n",
        "def _current_pairs():\n",
        "    return list(pairs_select.value)\n",
        "\n",
        "def _run_plots(_=None):\n",
        "    export_group_colors_to_global()\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "        mode = mode_radio.value\n",
        "        if mode == \"ref\":\n",
        "            ref = ref_dropdown.value if (ref_dropdown.value in sel_x) else _choose_ref_group(sel_x)\n",
        "            print(f\"Showing X groups: {sel_x}  |  reference for stars: {ref}\")\n",
        "        else:\n",
        "            pair_list = _current_pairs()\n",
        "            if not pair_list:\n",
        "                print(f\"Showing X groups: {sel_x}  |  no pairs selected (select at least one).\"); return\n",
        "            print(f\"Showing X groups: {sel_x}  |  pairs: {pair_list}\")\n",
        "\n",
        "        exclude = {}\n",
        "        metrics = [m for m in long_df[\"variable\"].dropna().unique() if m not in exclude]\n",
        "\n",
        "        for metric in metrics:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty:\n",
        "                print(f\"Skipping {metric} â€” no data for selected X groups.\"); continue\n",
        "            if mode == \"ref\":\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"ref\", ref_group=ref\n",
        "                )\n",
        "            else:\n",
        "                _plot_metric_clean(\n",
        "                    subset, metric,\n",
        "                    x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                    mode=\"pairs\", pair_list=_current_pairs()\n",
        "                )\n",
        "\n",
        "def _save_plots(_=None):\n",
        "    export_group_colors_to_global()\n",
        "    with out:\n",
        "        clear_output()\n",
        "        sel_x, color_map = _selected_x_and_colors()\n",
        "        if len(sel_x) < 1:\n",
        "            print(\"Select at least one X group.\"); return\n",
        "\n",
        "        mode = mode_radio.value\n",
        "        ref = ref_dropdown.value if (mode == \"ref\") else None\n",
        "        pair_list = _current_pairs() if (mode == \"pairs\") else None\n",
        "        if mode == \"pairs\" and not pair_list:\n",
        "            print(\"Select at least one pair before saving.\"); return\n",
        "\n",
        "        os.makedirs(\"metric_comparisons\", exist_ok=True)\n",
        "        saved = 0\n",
        "\n",
        "\n",
        "        for metric in [m for m in long_df[\"variable\"].dropna().unique()]:\n",
        "            subset = long_df[(long_df[\"variable\"] == metric) & (long_df[\"XGroup\"].isin(sel_x))]\n",
        "            if subset[\"value\"].dropna().empty: continue\n",
        "            fig = _plot_metric_clean(\n",
        "                subset, metric,\n",
        "                x_color_map={g: color_map[g] for g in sel_x if g in subset['XGroup'].unique()},\n",
        "                mode=mode, ref_group=ref, pair_list=pair_list, return_fig=True\n",
        "            )\n",
        "            safe = metric.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
        "            fig.savefig(f\"metric_comparisons/{safe}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "            plt.close(fig); saved += 1\n",
        "\n",
        "        if saved == 0:\n",
        "            print(\"No figures to save.\"); return\n",
        "        zipname = f\"metric_comparisons_{int(time.time())}.zip\"\n",
        "        shutil.make_archive(zipname.replace(\".zip\",\"\"), 'zip', \"metric_comparisons\")\n",
        "        if colab_files is not None:\n",
        "            colab_files.download(zipname)\n",
        "        print(f\"Saved {zipname}\")\n",
        "\n",
        "plot_btn.on_click(_run_plots)\n",
        "save_btn.on_click(_save_plots)\n",
        "\n",
        "# -----------------------\n",
        "# 9) Assemble compact UI (two columns)\n",
        "# -----------------------\n",
        "# Toggle visibility of ref vs pairs widgets\n",
        "def _toggle_controls(*_):\n",
        "    if mode_radio.value == \"ref\":\n",
        "        ref_dropdown.layout.display = \"\"\n",
        "        pairs_select.layout.display = \"none\"\n",
        "    else:\n",
        "        ref_dropdown.layout.display = \"none\"\n",
        "        pairs_select.layout.display = \"\"\n",
        "_toggle_controls()\n",
        "mode_radio.observe(lambda _: _toggle_controls(), names=\"value\")\n",
        "\n",
        "# Two-column layout container\n",
        "row = widgets.HBox(\n",
        "    [left_col, right_col],\n",
        "    layout=widgets.Layout(\n",
        "        justify_content=\"flex-start\",   # keep columns together\n",
        "        align_items=\"flex-start\",\n",
        "        gap=\"16px\",\n",
        "        width=\"auto\"\n",
        "    )\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='margin-bottom:6px'></h3>\"),\n",
        "    row,\n",
        "    out\n",
        "], layout=widgets.Layout(width=\"auto\"))\n",
        "export_group_colors_to_global()\n",
        "display(ui)\n",
        "\n",
        "# Auto-run once\n",
        "_run_plots()\n"
      ],
      "metadata": {
        "id": "dKKkCThnHE0H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grouped Demand Curves\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _basename(p):\n",
        "    s = str(p).replace(\"\\\\\", \"/\")\n",
        "    return s.split(\"/\")[-1]\n",
        "\n",
        "def log_logistic_norm(P, alpha, beta):\n",
        "    P = np.asarray(P, dtype=float)\n",
        "    return 100.0 / (1.0 + (P / float(alpha))**float(beta))  # Q0 = 100\n",
        "\n",
        "# ---------- preconditions ----------\n",
        "if \"feds\" not in globals() or not isinstance(feds, (list, tuple)) or len(feds) == 0:\n",
        "    raise RuntimeError(\"Need `feds`: list of FED logs (with 'Event' and 'Block_Pellet_Count').\")\n",
        "if \"files_to_group_both\" not in globals() or not isinstance(files_to_group_both, pd.DataFrame) or files_to_group_both.empty:\n",
        "    raise RuntimeError(\"Need `files_to_group_both` with columns 'filename' and 'XGroup'.\")\n",
        "\n",
        "# ---------- build group map ----------\n",
        "gdf = files_to_group_both.copy()\n",
        "gdf[\"filename\"] = gdf[\"filename\"].astype(str).map(_basename)\n",
        "gmap = gdf.drop_duplicates(\"filename\").set_index(\"filename\")[\"XGroup\"].to_dict()\n",
        "\n",
        "# ---------- fit per file and collect q, FR range ----------\n",
        "fit_rows, fr_all = [], []\n",
        "for i, df in enumerate(feds):\n",
        "    fname = _basename(getattr(df, \"name\", f\"File_{i}\"))\n",
        "\n",
        "    if {\"Event\", \"Block_Pellet_Count\"} - set(df.columns):\n",
        "        continue\n",
        "\n",
        "    pel = df[df[\"Event\"].astype(str) == \"Pellet\"].copy()\n",
        "    if pel.empty:\n",
        "        continue\n",
        "    pel[\"Block_Pellet_Count\"] = pd.to_numeric(pel[\"Block_Pellet_Count\"], errors=\"coerce\")\n",
        "    pel = pel.dropna(subset=[\"Block_Pellet_Count\"])\n",
        "    if pel.empty:\n",
        "        continue\n",
        "\n",
        "    fr_vals = pel[\"Block_Pellet_Count\"].astype(float).values\n",
        "    fr_all.extend(fr_vals.tolist())\n",
        "\n",
        "    # baseline B at cheapest FR; q = 100/B\n",
        "    min_fr = np.nanmin(fr_vals)\n",
        "    B = int(np.sum(fr_vals == min_fr))\n",
        "    if not (np.isfinite(B) and B > 0):\n",
        "        continue\n",
        "    q = 100.0 / B\n",
        "\n",
        "    # normalized demand points\n",
        "    counts = (pel.groupby(\"Block_Pellet_Count\").size()\n",
        "                .rename(\"PelletCount\").reset_index())\n",
        "    counts[\"PricePaid\"] = counts[\"Block_Pellet_Count\"].astype(float)\n",
        "    counts[\"Q_norm\"]    = counts[\"PelletCount\"].astype(float) * q\n",
        "    counts[\"P_norm\"]    = counts[\"PricePaid\"] * q\n",
        "    counts = counts.sort_values(\"P_norm\")\n",
        "\n",
        "    P = counts[\"P_norm\"].to_numpy(float)\n",
        "    Q = counts[\"Q_norm\"].to_numpy(float)\n",
        "\n",
        "    if np.unique(P[np.isfinite(P)]).size < 3:\n",
        "        continue  # not enough distinct price levels to fit robustly\n",
        "\n",
        "    # bounded fit\n",
        "    try:\n",
        "        alpha0 = np.nanmedian(P[np.isfinite(P)]) if np.isfinite(P).any() else 1.0\n",
        "        popt, _ = curve_fit(\n",
        "            log_logistic_norm, P, Q,\n",
        "            p0=[max(alpha0, 1e-3), 2.0],\n",
        "            bounds=([1e-6, 1e-3], [1e6, 50.0]),\n",
        "            maxfev=20000\n",
        "        )\n",
        "        a, b = map(float, popt)\n",
        "        if np.isfinite(a) and np.isfinite(b) and a > 0 and b > 0:\n",
        "            fit_rows.append({\n",
        "                \"File\": fname,\n",
        "                \"Group\": gmap.get(fname, \"UNASSIGNED\"),\n",
        "                \"alpha\": a,\n",
        "                \"beta\":  b,\n",
        "                \"q\":     q\n",
        "            })\n",
        "    except Exception:\n",
        "        pass  # skip files that fail to fit\n",
        "\n",
        "fits = pd.DataFrame(fit_rows)\n",
        "if fits.empty:\n",
        "    raise RuntimeError(\"No files produced valid fits (need â‰¥3 FR levels with pellets).\")\n",
        "\n",
        "# ---------- groups & colors ----------\n",
        "groups = sorted(fits[\"Group\"].unique().tolist())\n",
        "if \"group_colors\" not in globals() or not isinstance(group_colors, dict) or not group_colors:\n",
        "    pal = sns.color_palette(\"tab10\", n_colors=len(groups))\n",
        "    group_colors = {g: pal[i % len(pal)] for i, g in enumerate(groups)}\n",
        "\n",
        "# ---------- FR grid ----------\n",
        "if len(fr_all) == 0:\n",
        "    FR_left, FR_right = 1.0, 100.0\n",
        "else:\n",
        "    fr_all = pd.to_numeric(np.array(fr_all), errors=\"coerce\")\n",
        "    fr_all = fr_all[np.isfinite(fr_all)]\n",
        "    FR_left  = max(1.0, float(np.nanmin(fr_all))) if fr_all.size else 1.0\n",
        "    FR_right = float(np.nanmax(fr_all)) if fr_all.size else 100.0\n",
        "    if not np.isfinite(FR_right) or FR_right <= FR_left:\n",
        "        FR_right = max(FR_left * 10.0, 50.0)\n",
        "FR_grid = np.logspace(np.log10(FR_left), np.log10(FR_right), 400)\n",
        "\n",
        "# ---------- plot ----------\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "for g in groups:\n",
        "    sub = fits[fits[\"Group\"] == g]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    color = group_colors.get(g, None)\n",
        "\n",
        "    # per-file curves evaluated at P_norm = q * FR\n",
        "    Y, aFR_list, slopeFR_list = [], [], []\n",
        "    for _, r in sub.iterrows():\n",
        "        a, b, q = float(r[\"alpha\"]), float(r[\"beta\"]), float(r[\"q\"])\n",
        "        Y.append(log_logistic_norm(q * FR_grid, a, b))\n",
        "        aFR_list.append(a / q)                                 # Î± on FR scale\n",
        "        slopeFR_list.append((-(100.0 * b) / (4.0 * a)) * q)    # dQ/dFR at Î±\n",
        "\n",
        "    Y = np.asarray(Y)\n",
        "    mean_y = np.nanmean(Y, axis=0)\n",
        "    sem_y  = np.nanstd(Y, axis=0) / np.sqrt(Y.shape[0])\n",
        "\n",
        "    ax.plot(FR_grid, mean_y, lw=3, color=color, label=g)\n",
        "    ax.fill_between(FR_grid, mean_y - sem_y, mean_y + sem_y, color=color, alpha=0.30)\n",
        "\n",
        "    # on-curve one-line annotation\n",
        "    aFR_bar = float(np.nanmean(aFR_list)) if aFR_list else np.nan\n",
        "    b_bar   = float(np.nanmean(sub[\"beta\"])) if not sub.empty else np.nan\n",
        "    if np.isfinite(aFR_bar) and FR_left <= aFR_bar <= FR_right:\n",
        "        i, n = groups.index(g), len(groups)\n",
        "        jx = (i - (n - 1)/2) * (aFR_bar * 0.03)   # small horizontal jitter\n",
        "        jy = (i - (n - 1)/2) * 4.0                # small vertical jitter\n",
        "\n",
        "        ax.vlines(aFR_bar, ymin=0, ymax=50, color=color, linestyle=\"--\", lw=1.5, alpha=0.9)\n",
        "        ax.scatter([aFR_bar], [50], color=color, zorder=6, edgecolors=\"k\", linewidth=0.6)\n",
        "\n",
        "        dx = aFR_bar * 0.25\n",
        "        x1, x2 = aFR_bar - dx/2.0, aFR_bar + dx/2.0\n",
        "        m  = float(np.nanmean(slopeFR_list)) if slopeFR_list else 0.0\n",
        "        y1, y2 = 50 + jy + m * (x1 - aFR_bar), 50 + jy + m * (x2 - aFR_bar)\n",
        "        ax.plot([x1, x2], [y1, y2], color=color, lw=3.0, alpha=0.8)\n",
        "\n",
        "        label = f\"$\\\\alpha_{{}}$={aFR_bar:.2g}, $\\\\beta$={b_bar:.2g}\"\n",
        "        xm, ym = (x1 + x2)/2.0, (y1 + y2)/2.0\n",
        "        ax.annotate(label, xy=(xm, ym), xytext=(8 + jx, 0 + jy), textcoords=\"offset points\",\n",
        "                    color=color, fontsize=11, ha=\"left\", va=\"center\",\n",
        "                    bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
        "\n",
        "# axes & cosmetics\n",
        "ax.set_xscale(\"log\"); ax.set_xlim(FR_left, FR_right)\n",
        "ax.set_xlabel(\"Food Price (FR)\"); ax.set_ylabel(\"Consumption\")\n",
        "ax.set_yticks(np.arange(0, 101, 20)); ax.set_ylim(0, 105)\n",
        "ticks = np.array([1,2,3,5,10,15,20,30,40,50,75,100,150,200,300,400], float)\n",
        "ticks = ticks[(ticks >= FR_left) & (ticks <= FR_right)]\n",
        "ax.set_xticks(ticks); ax.set_xticklabels([str(int(x)) for x in ticks])\n",
        "ax.legend(title=\"Group\", frameon=False)\n",
        "sns.despine(); plt.tight_layout()\n",
        "# Embed TrueType fonts so text stays editable in Illustrator/Inkscape\n",
        "mpl.rcParams['pdf.fonttype'] = 42\n",
        "mpl.rcParams['ps.fonttype']  = 42\n",
        "\n",
        "\n",
        "from google.colab import files as gfiles  # alias to avoid shadowing any `files` variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose output folder and filename\n",
        "outdir = \"/content\"  # Colab's working dir is fine\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "fname = f\"demand_plot_{datetime.now():%Y%m%d_%H%M%S}.pdf\"\n",
        "\n",
        "# Get the figure you want to save (use the one that created `ax`)\n",
        "fig = ax.get_figure()  # or replace with a specific fig handle if you have one\n",
        "\n",
        "# Save and download\n",
        "path = os.path.join(outdir, fname)\n",
        "fig.savefig(path, format=\"pdf\", bbox_inches=\"tight\", transparent=True)\n",
        "gfiles.download(path)\n",
        "\n",
        "# Optional: show in notebook (won't affect the downloaded file)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ca_89TQMjUZN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Individual normalized demand curves fit with a (Q_0)-fixed log-logistic model**  \n",
        "Each plot shows **normalized** pellet consumption (Q) (gray points) as a function of normalized price (*P*)for a single animal. For each animal, we normalized the raw data by setting the lowest-price consumption to 100% and rescaling both axes: (q = 100/B) where (B) is consumption at the lowest price; then (P = Price X q) and (Q = Consumption X q). This forces all curves to start at (Q=100), removing trivial level differences and enabling like-for-like comparisons of elasticity across animals.\n",
        "\n",
        "The fitted demand curve (black line) was obtained by nonlinear least squares using a (Q_0)-fixed log-logistic model:![image.png](data:image/png;base64,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)\n",
        "\n",
        "**Where**  \n",
        "- alpha is the normalized price at which consumption is halved ((Q=50)),  \n",
        "- beta captures the local steepness of the decline around alpha.  \n",
        "**Each plot displays**:  \n",
        "\n",
        "- Gray points: normalized observed data,  \n",
        "- Black line: fitted demand curve,  \n",
        "- Red dashed line and red point: alpha half-max price, (Q=50),  \n",
        "- Light blue line: local slope at alpha.  \n",
        "**Interpretation**\n",
        "Because normalization fixes (Q_0=100), (Q_0) is no longer a free parameter and level differences are removed. Thus:  \n",
        "\n",
        "- alpha indexes cost sensitivity on a common scale; higher alpha indicates greater willingness to pay before consumption falls to half.  \n",
        "- beta quantifies how sharply consumption declines as (P) increases (larger beta â†’ steeper drop).    \n",
        "\n",
        "Optionally, we also report (Pmax) (vertical dotted line when included), defined as the price where elasticity equals (-1) and response output peaks."
      ],
      "metadata": {
        "id": "pp9_eOTdpJE6"
      }
    }
  ]
}