{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "js5njRNyTOpr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/PsygeneAnalyses/blob/PCA_analysis/Kravitz_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA plotting for PsyGene Mice Behavior"
      ],
      "metadata": {
        "id": "dGdV92l6yTac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "vsKySat9RA1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K6g2Cvhu6jAY"
      },
      "outputs": [],
      "source": [
        "####### Import Needed Libraries #######\n",
        "# Libraries for Machine Learnings/Modeling\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Libraries for importing\n",
        "import zipfile\n",
        "from google.colab import files # creates the ability for display uploads\n",
        "import tempfile\n",
        "import io\n",
        "import os\n",
        "# Data Frame libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for merging\n",
        "from functools import reduce\n",
        "\n",
        "from decorator import DEF\n",
        "\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions used through out the script"
      ],
      "metadata": {
        "id": "js5njRNyTOpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### DEFINE ALL FUNCTIONS #######\n",
        "\n",
        "### Session type Extraction ###\n",
        "# This code will extract the \"session_type\" column from each file\n",
        "def extract_session_type(df, file_name, fallback=\"Unknown\"):\n",
        "    # Follow Naming conventions so far\n",
        "    session_types = [\"weight\", \"bandit100\", \"fr1\", \"beam\", \"bandit80\",\n",
        "                     \"_pr_\", # we have to define pr surrounded by underscores to prevent random pr's\n",
        "                     \"individual_behavoir\", \"social_interaction\", \"nesting\"]\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    file_name = file_name.lower()\n",
        "    print(f\"Extracting session type for file: {file_name}\")\n",
        "    try:\n",
        "        #df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df = df\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        # Loop through common session like column names\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0].lower()\n",
        "            else:\n",
        "              # use the file name to determine what session type it is\n",
        "              matches = [c for c in session_types if c.lower() in file_name.lower()]\n",
        "              return(matches[0])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {csv_path}: {e}\")\n",
        "    return fallback\n",
        "\n",
        "### Retrieve dataframe profiles ###\n",
        "# expects a dataframe as an argument\n",
        "def df_prof(df):\n",
        "  # Get Dimensions\n",
        "  print(f\"Number of rows: {df.shape[0]}\")\n",
        "  print(f\"Number of columns: {df.shape[1]}\")\n",
        "\n",
        "  # Data types\n",
        "  df_prof = df.dtypes.rename(\"DataType\")\n",
        "\n",
        "  # Null percentages\n",
        "  df_null = (df.isna().sum() / len(df) * 100).rename(\"Percent_Null\")\n",
        "\n",
        "  # unique counts\n",
        "  df_uniq = df.nunique().rename(\"Number_Unique\")\n",
        "\n",
        "  # Merge into one DataFrame\n",
        "  merged_df = pd.concat([df_prof, df_null, df_uniq], axis=1).reset_index()\n",
        "  merged_df.columns = [\"Column\", \"DataType\", \"Percent_Null\", \"Number_Unique\"]\n",
        "  print(merged_df)\n",
        "\n",
        "\n",
        "\n",
        "### Clean Session Dataframes ###\n",
        "# Very fragile and expects the user to do manual investigation\n",
        "# could add detection for columns with null values\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def clean_session_types(session_dfs, all_drops, percent_drop = 5):\n",
        "  # Interate through large table and remove\n",
        "  for s, d in session_dfs.items():\n",
        "    print(f\"Cleaning Session Type: {s}\")\n",
        "\n",
        "    ### Conduct general data cleaning ###\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = all_drops, errors='ignore')\n",
        "\n",
        "    # Calculate the percentage of null values for each column\n",
        "    null_percentages = session_dfs[s].isnull().sum() * 100 / len(session_dfs[s])\n",
        "\n",
        "    # Identify columns where the null percentage is greater than 5%\n",
        "    columns_to_drop = null_percentages[null_percentages > percent_drop].index\n",
        "\n",
        "    # Drop the identified columns from the DataFrame\n",
        "    session_dfs[s] = session_dfs[s].drop(columns=columns_to_drop)\n",
        "    \"\"\"\n",
        "    ### Clean bandit80 specific data ###\n",
        "    if s.lower() in [\"bandit80\", \"bandit100\"]:\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = bandit_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['FED_StartDate'] = pd.to_datetime(session_dfs[s]['FED_StartDate'])\n",
        "\n",
        "    ### Clean weight specific data ###\n",
        "    if s.lower() in [\"weight\"]:\n",
        "\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = weight_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['DOB'] = pd.to_datetime(session_dfs[s]['DOB'])\n",
        "\n",
        "    # remove session type after all the cleaning\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = \"session_type\", errors='ignore')\n",
        "  \"\"\"\n",
        "  ### Return the session_dfs ###\n",
        "  return(session_dfs)\n",
        "\n",
        "\n",
        "\n",
        "### Define easy function for finding mode ###\n",
        "def mode_or_nan(x):\n",
        "    # handle ties or empty groups safely\n",
        "    return x.mode().iloc[0] if not x.mode().empty else None\n",
        "\n",
        "\n",
        "### Aggregate Session Dataframes ###\n",
        "# expects dictionary containing dataframes\n",
        "# Requires \"Mouse_ID\" Column\n",
        "# Will iterate through dataframes within the session dictionary passed\n",
        "#     and aggregate by Mouse_ID and ensure only 1 mouse is represented.\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def aggregate_session_types(session_dfs):\n",
        "    # Iterate across the session dictionary\n",
        "    for s, d in session_dfs.items():\n",
        "        print(f\"Aggregating Session Type: {s}\")\n",
        "\n",
        "        # Check to see if primary key is same length as number of rows in dataframe\n",
        "        if len(d) == d[\"Mouse_ID\"].nunique():\n",
        "            print(\"Number of Mice equal to Length of dataframe: No Aggregation Needed\")\n",
        "            continue\n",
        "\n",
        "        # Define numeric and categorical columns\n",
        "        numeric_cols = d.select_dtypes(include=['int64', 'float64']).columns\n",
        "        object_cols = d.select_dtypes(include=['object', 'category', 'datetime64']).columns\n",
        "\n",
        "        # perform aggregation\n",
        "        agg_dict = {\n",
        "            **{col: 'mean' for col in numeric_cols},\n",
        "            **{col: mode_or_nan for col in object_cols}\n",
        "        }\n",
        "\n",
        "        session_dfs[s] = (d.groupby('Mouse_ID', dropna=False, as_index=False)\n",
        "                          .agg(agg_dict))\n",
        "    return session_dfs"
      ],
      "metadata": {
        "id": "qyWLkGb6tRNG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Wanted Data Files\n",
        "Code based on Chantelle Murelle and Sebastian Alves\n",
        "\n",
        "The following code will iteritvly read all csv's within the zipped file and seperate them into a dictionary based on the various recordings. this will be used for processing the data in a session type manner.\n",
        "\n",
        "This could include L3 for all various recordings:\n",
        "1.   Weight\n",
        "2.   Bandit100_0\n",
        "3.   FR1\n",
        "4.   BEAM\n",
        "5.   Bandit80_20\n",
        "6.   PR\n",
        "7.   Individual Behavoir\n",
        "8.   Social Interaction\n",
        "9.   Nesting\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbh3oEgXrujO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Import Wanted Data Files ########\n",
        "# Code based on Chantelle Murelle and Sebastian Alves\n",
        "\n",
        "### reset cache ###\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "session_dict, loaded_files, session_types = [], [], []\n",
        "\n",
        "### Define the upload UI element ###\n",
        "uploaded = files.upload()\n",
        "\n",
        "### Create Data dictonary and containers ###\n",
        "session_dict = {} # for storing by session type\n",
        "loaded_files = [] # for files\n",
        "\n",
        "### Loop through the uploaded files ###\n",
        "# This expects a zippped file exclusivly\n",
        "for name, data in uploaded.items():\n",
        "  with zipfile.ZipFile(io.BytesIO(data)) as zf: # use zipfile to \"unzip\"\n",
        "    for zi in zf.infolist(): # iterate through the zipped files\n",
        "\n",
        "\n",
        "      # skip non handled file types\n",
        "      # Skip non-csv/xlsx files\n",
        "      if not (zi.filename.endswith(\".csv\") or zi.filename.endswith(\".xlsx\")):\n",
        "        print(f\"Skipping: {zi.filename}\")\n",
        "        continue\n",
        "      print(f\"\\nProcessing: {zi.filename}\")\n",
        "\n",
        "      # define filetype\n",
        "      file_type = \"csv\" if zi.filename.endswith(\".csv\") else \"xlsx\"\n",
        "\n",
        "      # read the file in\n",
        "      file_data = zf.read(zi)\n",
        "\n",
        "      # write temporary file into memory\n",
        "      suffix = \".csv\" if file_type == \"csv\" else \".xlsx\"\n",
        "      with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=suffix, delete=False) as tmp:\n",
        "        tmp.write(file_data)\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "      # Define try catch statment and read into memory\n",
        "      try:\n",
        "        if file_type == \"csv\":\n",
        "          df = pd.read_csv(tmp_path)\n",
        "        else:\n",
        "          df = pd.read_excel(tmp_path)\n",
        "        #df[\"source_file\"] = zi.filename\n",
        "        session_type = extract_session_type(df, zi.filename)\n",
        "        # rewrite session\n",
        "        df[\"session_type\"] = session_type\n",
        "        print(session_type)\n",
        "\n",
        "        # Add to the data dictionary based on session type\n",
        "        if session_type not in session_dict:\n",
        "          session_dict[session_type] = []\n",
        "        session_dict[session_type].append(df)\n",
        "      except Exception as e: # throw error on failure to load\n",
        "        print(f\"Error loading {zi.filename}: {e}\")\n",
        "      finally: # remove the temporary path\n",
        "        os.remove(tmp_path)\n",
        "\n",
        "\n",
        "\n",
        "### Combine dataframes for each session ###\n",
        "# Convert each list of dfs into one merged dataframe per session type\n",
        "session_dfs = {k: pd.concat(v, ignore_index=True) for k, v in session_dict.items()}\n",
        "\n",
        "session_dfs.pop('weight', None)\n",
        "#session_dfs.pop('bandit100', None)\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in session_dfs.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(session_dfs[s])"
      ],
      "metadata": {
        "id": "8pJCTyGLzH2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin to clean data.\n",
        "Define what columns for what seesion types will be dropped. This should be based on nullness and should seek to keep as large an N as possible.\n",
        "\n",
        "Additionally, columns that are believe to add little to no context could be dropped."
      ],
      "metadata": {
        "id": "-dM7b0alrTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### BEGIN DATA CLEANING AND SCRUBBING #######\n",
        "\n",
        "\n",
        "### Strict Drop Lists ###\n",
        "all_drops = [\"filename\", \"filename_md\", \"Session_type\", \"match_status\",\n",
        "               \"Primary\", \"Autoexcluder\",\n",
        "               \"BEAM_EX\", \"Bandit100_EX\", \"Bandit80_EX\", \"FR1_EX\", \"PR1_EX\"]\n",
        "bandit_drops = [\"Weight1_Friday\", \"Weight2_Monday\", \"Weight3_Friday\",\n",
        "                \"Weight4_Monday\", \"Weight5_Friday\", \"Weight6_SI\",\n",
        "                \"Stim_Partner\", \"Stim_Weight\", \"DOB\"]\n",
        "weight_drops = [\"BEAM End (2 day, internal)\", \"FED_StartDate\",\n",
        "                \"Weight1_Friday\", \"Weight2_Monday\"]\n",
        "\n",
        "\n",
        "dfs_clean = aggregate_session_types(session_dfs)\n",
        "\n",
        "\n",
        "dfs_clean = clean_session_types(session_dfs=dfs_clean,\n",
        "                                all_drops=all_drops,\n",
        "                                bandit_drops=bandit_drops,\n",
        "                                weight_drops= weight_drops)\n",
        "\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in dfs_clean.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(dfs_clean[s])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WZ6yTNtP7X-D",
        "outputId": "771e8740-843d-4178-b51d-449d7e17e98b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregating Session Type: bandit80\n",
            "Aggregating Session Type: fr1\n",
            "Number of Mice equal to Length of dataframe: No Aggregation Needed\n",
            "Aggregating Session Type: bandit100\n",
            "Number of Mice equal to Length of dataframe: No Aggregation Needed\n",
            "Aggregating Session Type: beam\n",
            "Number of Mice equal to Length of dataframe: No Aggregation Needed\n",
            "Aggregating Session Type: pr1\n",
            "Number of Mice equal to Length of dataframe: No Aggregation Needed\n",
            "Cleaning Session Type: bandit80\n",
            "Cleaning Session Type: fr1\n",
            "Cleaning Session Type: bandit100\n",
            "Cleaning Session Type: beam\n",
            "Cleaning Session Type: pr1\n",
            "\n",
            "Session 'bandit80' → 218 rows, 23 cols\n",
            "Number of rows: 218\n",
            "Number of columns: 23\n",
            "                         Column        DataType  Percent_Null  Number_Unique\n",
            "0                       Gene_ID         float64      0.000000              5\n",
            "1                          BEAM         float64      0.000000             69\n",
            "2             Win-stay_Bandit80         float64      0.000000            216\n",
            "3           Lose-shift_Bandit80         float64      0.000000            194\n",
            "4         PeakAccuracy_Bandit80         float64      0.458716            186\n",
            "5        Total_pellets_Bandit80         float64      0.000000            146\n",
            "6          Total_pokes_Bandit80         float64      0.458716            174\n",
            "7       PokesPerPellet_Bandit80         float64      0.458716            214\n",
            "8        RetrievalTime_Bandit80         float64      0.000000             76\n",
            "9             PokeTime_Bandit80         float64      0.000000             44\n",
            "10       daily pellets_Bandit80         float64      0.000000            218\n",
            "11        Win-stay_Day_Bandit80         float64      0.000000            207\n",
            "12      Win-stay_Night_Bandit80         float64      0.000000            216\n",
            "13      Lose-shift_Day_Bandit80         float64      0.000000            137\n",
            "14    Lose-shift_Night_Bandit80         float64      0.000000            169\n",
            "15    PeakAccuracy_Day_Bandit80         float64      0.458716            129\n",
            "16  PeakAccuracy_Night_Bandit80         float64      0.458716            164\n",
            "17                     Mouse_ID          object      0.000000            218\n",
            "18                         Gene          object      0.000000              5\n",
            "19                    Genotype$          object      0.000000              4\n",
            "20                         Sex$          object      0.000000              2\n",
            "21                         FED3          object      0.000000            121\n",
            "22                FED_StartDate  datetime64[ns]      0.000000              6\n",
            "\n",
            "Session 'fr1' → 211 rows, 42 cols\n",
            "Number of rows: 211\n",
            "Number of columns: 42\n",
            "                         Column DataType  Percent_Null  Number_Unique\n",
            "0                      Mouse_ID   object      0.000000            211\n",
            "1                          Sex$   object      0.000000              2\n",
            "2                     Genotype$   object      0.000000              4\n",
            "3                          Gene   object      0.000000              5\n",
            "4                       Gene_ID    int64      0.000000              5\n",
            "5                          FED3   object      0.000000            125\n",
            "6                          BEAM    int64      0.000000             69\n",
            "7                   Pellets_FR1  float64      0.000000             97\n",
            "8                 Left_Poke_FR1  float64      0.000000             97\n",
            "9                Right_Poke_FR1  float64      0.000000             95\n",
            "10              Total_Pokes_FR1  float64      0.000000            112\n",
            "11                 Accuracy_FR1  float64      0.000000            209\n",
            "12           PokesPerPellet_FR1  float64      0.000000            211\n",
            "13            RetrievalTime_FR1  float64      0.000000             61\n",
            "14      InterPelletInterval_FR1  float64      0.000000             96\n",
            "15                 PokeTime_FR1  float64      0.000000             31\n",
            "16             %MealPellets_FR1  float64      0.000000            206\n",
            "17          %GrazingPellets_FR1  float64      0.000000            206\n",
            "18                 NumMeals_FR1  float64      0.000000             29\n",
            "19              AvgMealSize_FR1  float64      0.000000            182\n",
            "20          AvgMealDuration_FR1  float64      0.000000            210\n",
            "21           RecordingHours_FR1  float64      0.000000            210\n",
            "22             MealsPerHour_FR1  float64      0.000000            211\n",
            "23            Daily_Pellets_FR1  float64      0.000000            211\n",
            "24    Left Poke with Pellet_FR1  float64      0.000000             96\n",
            "25  Within_meal_pellet_rate_FR1  float64      0.000000            210\n",
            "26         %MealPellets_Day_FR1  float64      0.000000            176\n",
            "27      %GrazingPellets_Day_FR1  float64      0.000000            176\n",
            "28              Pellets_Day_FR1  float64      0.000000             71\n",
            "29             NumMeals_Day_FR1  float64      0.000000             15\n",
            "30          AvgMealSize_Day_FR1  float64      0.473934            102\n",
            "31      AvgMealDuration_Day_FR1  float64      0.473934            200\n",
            "32         MealsPerHour_Day_FR1  float64      0.000000            211\n",
            "33             Accuracy_Day_FR1  float64      0.000000            192\n",
            "34       %MealPellets_Night_FR1  float64      0.000000            203\n",
            "35    %GrazingPellets_Night_FR1  float64      0.000000            203\n",
            "36            Pellets_Night_FR1  float64      0.000000             81\n",
            "37           NumMeals_Night_FR1  float64      0.000000             24\n",
            "38        AvgMealSize_Night_FR1  float64      0.000000            169\n",
            "39    AvgMealDuration_Night_FR1  float64      0.000000            209\n",
            "40       MealsPerHour_Night_FR1  float64      0.000000            211\n",
            "41           Accuracy_Night_FR1  float64      0.000000            206\n",
            "\n",
            "Session 'bandit100' → 155 rows, 23 cols\n",
            "Number of rows: 155\n",
            "Number of columns: 23\n",
            "                          Column        DataType  Percent_Null  Number_Unique\n",
            "0                       Mouse_ID          object           0.0            155\n",
            "1                           Gene          object           0.0              4\n",
            "2                        Gene_ID           int64           0.0              4\n",
            "3                           Sex$          object           0.0              2\n",
            "4                      Genotype$          object           0.0              3\n",
            "5                           FED3          object           0.0            108\n",
            "6                           BEAM           int64           0.0             69\n",
            "7                  FED_StartDate  datetime64[ns]           0.0              5\n",
            "8             Win-stay_Bandit100         float64           0.0            153\n",
            "9           Lose-shift_Bandit100         float64           0.0            140\n",
            "10        PeakAccuracy_Bandit100         float64           0.0            134\n",
            "11       Total_pellets_Bandit100         float64           0.0            123\n",
            "12         Total_pokes_Bandit100         float64           0.0            134\n",
            "13      PokesPerPellet_Bandit100         float64           0.0            155\n",
            "14       RetrievalTime_Bandit100         float64           0.0             56\n",
            "15            PokeTime_Bandit100         float64           0.0             25\n",
            "16       daily pellets_Bandit100         float64           0.0            155\n",
            "17        Win-stay_Day_Bandit100         float64           0.0            147\n",
            "18      Win-stay_Night_Bandit100         float64           0.0            153\n",
            "19      Lose-shift_Day_Bandit100         float64           0.0            102\n",
            "20    Lose-shift_Night_Bandit100         float64           0.0            128\n",
            "21    PeakAccuracy_Day_Bandit100         float64           0.0            108\n",
            "22  PeakAccuracy_Night_Bandit100         float64           0.0            131\n",
            "\n",
            "Session 'beam' → 73 rows, 12 cols\n",
            "Number of rows: 73\n",
            "Number of columns: 12\n",
            "                   Column DataType  Percent_Null  Number_Unique\n",
            "0                Mouse_ID   object           0.0             73\n",
            "1                Genotype   object           0.0              2\n",
            "2                    Gene   object           0.0              2\n",
            "3                     Sex   object           0.0              2\n",
            "4                 Night_Z  float64           0.0             72\n",
            "5                   Day_Z  float64           0.0             72\n",
            "6                   MESOR  float64           0.0             72\n",
            "7               Amplitude  float64           0.0             72\n",
            "8     Acrophase_ZT_signed  float64           0.0             72\n",
            "9   Acrophase_ZT_unsigned  float64           0.0             72\n",
            "10   Acrophase_Clock_Hour  float64           0.0             72\n",
            "11             Cosinor_R2  float64           0.0             72\n",
            "\n",
            "Session 'pr1' → 156 rows, 34 cols\n",
            "Number of rows: 156\n",
            "Number of columns: 34\n",
            "                  Column DataType  Percent_Null  Number_Unique\n",
            "0               Mouse_ID   object      0.000000            156\n",
            "1                   Gene   object      0.000000              4\n",
            "2                Gene_ID  float64      0.000000              4\n",
            "3                   Sex$   object      0.000000              2\n",
            "4              Genotype$   object      0.000000              3\n",
            "5                   FED3   object      0.000000            109\n",
            "6                   BEAM  float64      0.000000             67\n",
            "7                    DOB   object      0.000000             22\n",
            "8          FED_StartDate   object     31.410256              4\n",
            "9         Weight1_Friday  float64      9.615385            127\n",
            "10        Weight2_Monday  float64     25.000000            114\n",
            "11        Weight3_Friday  float64      0.000000            143\n",
            "12        Weight4_Monday  float64      0.000000            145\n",
            "13        Weight5_Friday  float64      0.000000            141\n",
            "14            Weight6_SI  float64      0.000000            138\n",
            "15          Stim_Partner   object      0.000000             77\n",
            "16           Stim_Weight  float64      0.000000            121\n",
            "17         Left_Poke_PR1  float64      0.000000            150\n",
            "18        Right_Poke_PR1  float64      0.000000            137\n",
            "19       Total_Pokes_PR1  float64      0.000000            150\n",
            "20          Accuracy_PR1  float64      0.000000            156\n",
            "21    PokesPerPellet_PR1  float64      0.000000            156\n",
            "22  MedianBreakPoint_PR1  float64      0.000000             29\n",
            "23    Numberofblocks_PR1  float64      0.000000             25\n",
            "24     daily pellets_PR1  float64      0.000000            156\n",
            "25     Demand_Q0_raw_PR1  float64      3.846154            150\n",
            "26  Demand_alpha_raw_PR1  float64      3.846154            150\n",
            "27   Demand_beta_raw_PR1  float64      3.846154            150\n",
            "28   Demand_alpha_FR_PR1  float64      3.846154            150\n",
            "29    Demand_beta_FR_PR1  float64      3.846154            150\n",
            "30        NF1_003_140_10  float64    100.000000              0\n",
            "31   2025-05-17 00:00:00  float64    100.000000              0\n",
            "32    SHANK3B_004_114_78  float64    100.000000              0\n",
            "33   2025-05-13 00:00:00  float64    100.000000              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge and Prepare Columns\n",
        "\n",
        "##Begin the merge of the disperate types of metric data.\n",
        "\n",
        "Continues cleaning and data formatting including:\n",
        "\n",
        "*   Remove any rows that have any null values present.\n",
        "*   Calculate # of Days columns (if date times are present).\n"
      ],
      "metadata": {
        "id": "9sPLN44yyeL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Merge the list of dataframes #######\n",
        "\n",
        "# Define the features that will be merged on\n",
        "merge_keys = [\"Mouse_ID\", \"Gene\", \"Sex$\", \"Genotype$\"]\n",
        "\n",
        "#print(merge_keys)\n",
        "\n",
        "### Add prefixs for columns ###\n",
        "#Merge the dataframes and add a prefix to track where it came from\n",
        "# add prefixs to the dataframes based  on session type to track them\n",
        "prefixed_dfs = [\n",
        "    df.rename(columns=lambda c: f\"{s}_{c}\" if c not in merge_keys else c)\n",
        "    for s, df in dfs_clean.items()\n",
        "]\n",
        "\n",
        "### Merge the dataframes ###\n",
        "# Merge based on multiple grouping columns\n",
        "merged = reduce(\n",
        "    lambda left, right: pd.merge(left, right, on=merge_keys, how=\"outer\"),\n",
        "    prefixed_dfs\n",
        ")\n",
        "\n",
        "\n",
        "### Remove any rows that contain null values ###\n",
        "# Replace textual 'nan' with real NaN and assign\n",
        "replaced = merged.replace('nan', np.nan)\n",
        "# Keep only rows that have NO nulls in any column\n",
        "df_all = replaced[~replaced.isnull().any(axis=1)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "### Deal with the dates ###\n",
        "# by representing how old each mouse is (days) for various tasks\n",
        "dob_col = df_all.columns[df_all.columns.str.contains(\"DOB\")]\n",
        "if not dob_col.empty:\n",
        "  # Extract column\n",
        "  dob_col = dob_col[0]\n",
        "  print(f\"Found DOB Column: {dob_col} \\nCalculating days old for tasks\")\n",
        "  fed_starts = df_all.columns[df_all.columns.str.contains(\"FED_StartDate\")]\n",
        "\n",
        "  # Convert DOB column once to datetime\n",
        "  dob_dt = pd.to_datetime(df_all[dob_col], errors='coerce')\n",
        "\n",
        "  # Loop through each FED_StartDate column and calculate days difference\n",
        "  for fed_col in fed_starts:\n",
        "    new_col_name = f\"days_old_for_{fed_col}\"\n",
        "    df_all[new_col_name] = (\n",
        "        pd.to_datetime(df_all[fed_col], errors='coerce') - dob_dt\n",
        "        ).dt.days\n",
        "    print(f\"→ Created column: {new_col_name}\")\n",
        "\n",
        "\n",
        "### Remove any datetime columns ###\n",
        "datetime_cols = df_all.select_dtypes(include=['datetime64']).columns\n",
        "df_all.drop(columns = datetime_cols, inplace=True)\n",
        "\n",
        "\n",
        "### Check the data ###\n",
        "print(df_all.columns.values)\n",
        "df_prof(df_all)\n",
        "#df_all.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Save CSV ----------\n",
        "#csv_name = f\"PRmetrics_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "csv_name = f\"all_merged_dataframes.csv\"\n",
        "\n",
        "down_df = df_all.to_csv(csv_name, index=False)\n",
        "#display(HTML(f\"<b>✓ Saved PR metrics CSV to:</b> <code>{csv_name}</code>\"))\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "# Optional download button (works in Colab)\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name)}</code>…\"\n",
        "        gfiles.download(csv_name)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "JQrmiaETytxY",
        "outputId": "a19f6e16-f61c-4d7f-a666-e0d37ea343ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Gene_ID'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3140383218.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m### Merge the dataframes ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Merge based on multiple grouping columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m merged = reduce(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprefixed_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3140383218.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Merge based on multiple grouping columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m merged = reduce(\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprefixed_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Gene_ID'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepares data for Application to Multiple ML Models\n",
        "Should prepare data in theory for:\n",
        "*   PCA (Maybe)\n",
        "*   RandomForestModels\n",
        "\n",
        "This will include:\n",
        "1.   One-Hot Encoding\n",
        "2.   Split of Training and Test Data\n",
        "3.   Normalization of Data   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5grk05ssuHy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Prepare data for multiple ML approaches #######\n",
        "\n",
        "### Define the classifier for prediction ###\n",
        "pred_col = [\"Gene\"]\n",
        "\n",
        "### Define known categorical columns ###\n",
        "known_objects = [\"Sex$\", \"Genotype$\"]\n",
        "\n",
        "### Identify all FED and BEAM columns ###\n",
        "# are also categorical\n",
        "search_terms = ['FED3', 'BEAM']\n",
        "fed_beam_cols = [col for col in df_all.columns if any(term in col for term in search_terms)]\n",
        "known_objects.extend(fed_beam_cols)\n",
        "\n",
        "### Cast categorical columns ###\n",
        "df_all[known_objects] = df_all[known_objects].astype('object')\n",
        "\n",
        "# Define ID columns (Columns to Drop)\n",
        "id_cols = [\"Gene_ID\", \"Mouse_ID\"]\n",
        "id_cols.extend(pred_col)\n",
        "\n",
        "# Define X and y\n",
        "y = df_all[pred_col] # Currently not encoded\n",
        "X = df_all.drop(columns=id_cols)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# One-hot encode categorical features\n",
        "print(f\"One-hot encoding the following columns: {list(cat_cols)}\")\n",
        "X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "\n",
        "# Align columns between train and test\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Scale data\n",
        "# find numeric\n",
        "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "# find boolean\n",
        "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
        "\n",
        "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.fit_transform(X_test[num_cols])\n",
        "\n",
        "# Glimpse training data\n",
        "X_train.head()\n",
        "print(df_prof(X_train))"
      ],
      "metadata": {
        "id": "sdZFggMYA_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format PCA Data"
      ],
      "metadata": {
        "id": "pdINveHO5r9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Format data into PCA ready format #######\n",
        "\n",
        "### Drop Keys and object columns ###\n",
        "pca_df = df_all.drop(columns = merge_keys)\n",
        "drops = pca_df.select_dtypes(include=['object', 'bool']).columns\n",
        "pca_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "features = pca_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(len(features))\n",
        "print(features)\n",
        "\n",
        "### Standardize the scale ###\n",
        "x = pca_df.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "pca_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "\n",
        "### Check the data ###\n",
        "df_prof(pca_df)\n",
        "pca_df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "DHf1ZMvH5uk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conduct PCA Plotting Colored by Various Categoricals"
      ],
      "metadata": {
        "id": "dcTvDzFARmrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### PCA Plotting #######\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "\n",
        "clustering_df_pca = pd.DataFrame(data = clustering_df_pca,\n",
        "                                 columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "print('Explained variability per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "#plt.figure()\n",
        "#plt.figure(figsize=(5,5))\n",
        "#plt.xticks(fontsize=12)\n",
        "#plt.yticks(fontsize=14)\n",
        "#plt.xlabel('Principal Component - 1',fontsize=20)\n",
        "#plt.ylabel('Principal Component - 2',fontsize=20)\n",
        "#plt.title(\"Principal Component Analysis Behavoirs\", fontsize=20)\n",
        "#targets = df_all['Gene'].unique()\n",
        "#colors = ['r', 'g', 'b', 'y', 'p']\n",
        "#for target, color in zip(targets,colors):\n",
        "#    indicesToKeep = df_all['Gene'] == target\n",
        "#    plt.scatter(clustering_df_pca.loc[indicesToKeep, 'principal component 1'],\n",
        "#                clustering_df_pca.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
        "#\n",
        "#plt.legend(targets,prop={'size': 8})\n",
        "\n",
        "\n",
        "# Colummns for coloring\n",
        "categorical_columns = ['Gene', 'Sex$', 'Genotype$']\n",
        "\n",
        "for cat_col in categorical_columns:\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.title(f\"PCA colored by {cat_col}\", fontsize=16)\n",
        "    plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
        "    plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
        "\n",
        "    ### Get unique groups for column ###\n",
        "    targets = df_all[cat_col].dropna().unique()\n",
        "\n",
        "    # Generate colors\n",
        "    cmap = plt.cm.get_cmap('viridis', len(targets))\n",
        "    colors = [cmap(i) for i in range(len(targets))]\n",
        "\n",
        "    ### Scatter for each group ###\n",
        "    for i, target in enumerate(targets):\n",
        "        indicesToKeep = df_all[cat_col] == target\n",
        "        plt.scatter(\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 1'],\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 2'],\n",
        "            color=colors[i],\n",
        "            s=50,\n",
        "            alpha=0.7,\n",
        "            edgecolors='k',\n",
        "            label=str(target)\n",
        "        )\n",
        "\n",
        "    plt.legend(title=cat_col, fontsize=9, title_fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hSKHHmV1Mtll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create K-Cluster Elbow Plot to Prepare Discovery of Clusters"
      ],
      "metadata": {
        "id": "6HVhBM98RXk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Explore how many K clusters #######\n",
        "\n",
        "# Initialize variables for Elbow Method\n",
        "inertia = []\n",
        "K_range = range(2, 11) # Testing 2 to 10 clusters\n",
        "\n",
        "# Iterate over potential numer of clusters\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=25).fit(pca_df)\n",
        "    inertia.append(kmeans.inertia_) # Within-cluster sum of squares\n",
        "# Plot the Elbow Method\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(K_range, inertia, 'o-', color='blue')\n",
        "plt.xlabel('Number of Clusters (k)', fontsize = 15)\n",
        "plt.ylabel('Within-Cluster Sum of Squares)', fontsize=15)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "zP38UtpYKhX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform the clustering using K from Above"
      ],
      "metadata": {
        "id": "-FTts6TZsbz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform k-means clustering and save\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(pca_df)\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "\n",
        "# Plot our pca data colored by clusters\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(clustering_df_pca[:, 0], clustering_df_pca[:, 1], c=labels,\n",
        "            cmap='viridis', s=30)\n",
        "plt.title('K-Means Clusters (PCA 2D Projection)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')"
      ],
      "metadata": {
        "id": "yrqfTcVvsiYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}