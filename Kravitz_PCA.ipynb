{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "js5njRNyTOpr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/PsygeneAnalyses/blob/PCA_analysis/Kravitz_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA plotting for PsyGene Mice Behavior"
      ],
      "metadata": {
        "id": "dGdV92l6yTac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "vsKySat9RA1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K6g2Cvhu6jAY"
      },
      "outputs": [],
      "source": [
        "####### Import Needed Libraries #######\n",
        "# Libraries for Machine Learnings/Modeling\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Libraries for importing\n",
        "import zipfile\n",
        "from google.colab import files # creates the ability for display uploads\n",
        "import tempfile\n",
        "import io\n",
        "import os\n",
        "# Data Frame libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for merging\n",
        "from functools import reduce\n",
        "\n",
        "from decorator import DEF\n",
        "\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions used through out the script"
      ],
      "metadata": {
        "id": "js5njRNyTOpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### DEFINE ALL FUNCTIONS #######\n",
        "\n",
        "### Session type Extraction ###\n",
        "# This code will extract the \"session_type\" column from each file\n",
        "def extract_session_type(df, file_name, fallback=\"Unknown\"):\n",
        "    # Follow Naming conventions so far\n",
        "    session_types = [\"weight\", \"bandit100\", \"fr1\", \"beam\", \"bandit80\",\n",
        "                     \"_pr_\", # we have to define pr surrounded by underscores to prevent random pr's\n",
        "                     \"individual_behavoir\", \"social_interaction\", \"nesting\"]\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    file_name = file_name.lower()\n",
        "    print(f\"Extracting session type for file: {file_name}\")\n",
        "    try:\n",
        "        #df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df = df\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        # Loop through common session like column names\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0].lower()\n",
        "            else:\n",
        "              # use the file name to determine what session type it is\n",
        "              matches = [c for c in session_types if c.lower() in file_name.lower()]\n",
        "              return(matches[0])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {csv_path}: {e}\")\n",
        "    return fallback\n",
        "\n",
        "### Retrieve dataframe profiles ###\n",
        "# expects a dataframe as an argument\n",
        "def df_prof(df):\n",
        "  # Get Dimensions\n",
        "  print(f\"Number of rows: {df.shape[0]}\")\n",
        "  print(f\"Number of columns: {df.shape[1]}\")\n",
        "\n",
        "  # Data types\n",
        "  df_prof = df.dtypes.rename(\"DataType\")\n",
        "\n",
        "  # Null percentages\n",
        "  df_null = (df.isna().sum() / len(df) * 100).rename(\"Percent_Null\")\n",
        "\n",
        "  # unique counts\n",
        "  df_uniq = df.nunique().rename(\"Number_Unique\")\n",
        "\n",
        "  # Merge into one DataFrame\n",
        "  merged_df = pd.concat([df_prof, df_null, df_uniq], axis=1).reset_index()\n",
        "  merged_df.columns = [\"Column\", \"DataType\", \"Percent_Null\", \"Number_Unique\"]\n",
        "  print(merged_df)\n",
        "\n",
        "\n",
        "\n",
        "### Clean Session Dataframes ###\n",
        "# Very fragile and expects the user to do manual investigation\n",
        "# could add detection for columns with null values\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def clean_session_types(session_dfs, all_drops, percent_drop = 5):\n",
        "  # Interate through large table and remove\n",
        "  for s, d in session_dfs.items():\n",
        "    print(f\"Cleaning Session Type: {s}\")\n",
        "\n",
        "    # remove \"$\" from column names\n",
        "    session_dfs[s].columns = session_dfs[s].columns.str.replace('$', '')\n",
        "\n",
        "    ### Conduct general data cleaning ###\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = all_drops, errors='ignore')\n",
        "\n",
        "    # Calculate the percentage of null values for each column\n",
        "    null_percentages = session_dfs[s].isnull().sum() * 100 / len(session_dfs[s])\n",
        "\n",
        "    # Identify columns where the null percentage is greater than 5%\n",
        "    columns_to_drop = null_percentages[null_percentages > percent_drop].index\n",
        "\n",
        "    # Drop the identified columns from the DataFrame\n",
        "    session_dfs[s] = session_dfs[s].drop(columns=columns_to_drop)\n",
        "    \"\"\"\n",
        "    ### Clean bandit80 specific data ###\n",
        "    if s.lower() in [\"bandit80\", \"bandit100\"]:\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = bandit_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['FED_StartDate'] = pd.to_datetime(session_dfs[s]['FED_StartDate'])\n",
        "\n",
        "    ### Clean weight specific data ###\n",
        "    if s.lower() in [\"weight\"]:\n",
        "\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = weight_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['DOB'] = pd.to_datetime(session_dfs[s]['DOB'])\n",
        "\n",
        "    # remove session type after all the cleaning\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = \"session_type\", errors='ignore')\n",
        "  \"\"\"\n",
        "  ### Return the session_dfs ###\n",
        "  return(session_dfs)\n",
        "\n",
        "\n",
        "\n",
        "### Define easy function for finding mode ###\n",
        "def mode_or_nan(x):\n",
        "    # handle ties or empty groups safely\n",
        "    return x.mode().iloc[0] if not x.mode().empty else None\n",
        "\n",
        "\n",
        "### Aggregate Session Dataframes ###\n",
        "# expects dictionary containing dataframes\n",
        "# Requires \"Mouse_ID\" Column\n",
        "# Will iterate through dataframes within the session dictionary passed\n",
        "#     and aggregate by Mouse_ID and ensure only 1 mouse is represented.\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def aggregate_session_types(session_dfs):\n",
        "    # Iterate across the session dictionary\n",
        "    for s, d in session_dfs.items():\n",
        "        print(f\"Aggregating Session Type: {s}\")\n",
        "\n",
        "        # Check to see if primary key is same length as number of rows in dataframe\n",
        "        if len(d) == d[\"Mouse_ID\"].nunique():\n",
        "            print(\"Number of Mice equal to Length of dataframe: No Aggregation Needed\")\n",
        "            continue\n",
        "\n",
        "        # Define numeric and categorical columns\n",
        "        numeric_cols = d.select_dtypes(include=['int64', 'float64']).columns\n",
        "        object_cols = d.select_dtypes(include=['object', 'category', 'datetime64']).columns\n",
        "\n",
        "        # perform aggregation\n",
        "        agg_dict = {\n",
        "            **{col: 'mean' for col in numeric_cols},\n",
        "            **{col: mode_or_nan for col in object_cols}\n",
        "        }\n",
        "\n",
        "        session_dfs[s] = (d.groupby('Mouse_ID', dropna=False, as_index=False)\n",
        "                          .agg(agg_dict))\n",
        "    return session_dfs"
      ],
      "metadata": {
        "id": "qyWLkGb6tRNG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Wanted Data Files\n",
        "Code based on Chantelle Murelle and Sebastian Alves\n",
        "\n",
        "The following code will iteritvly read all csv's within the zipped file and seperate them into a dictionary based on the various recordings. this will be used for processing the data in a session type manner.\n",
        "\n",
        "This could include L3 for all various recordings:\n",
        "1.   Weight\n",
        "2.   Bandit100_0\n",
        "3.   FR1\n",
        "4.   BEAM\n",
        "5.   Bandit80_20\n",
        "6.   PR\n",
        "7.   Individual Behavoir\n",
        "8.   Social Interaction\n",
        "9.   Nesting\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbh3oEgXrujO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Import Wanted Data Files ########\n",
        "# Code based on Chantelle Murelle and Sebastian Alves\n",
        "\n",
        "### reset cache ###\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "session_dict, loaded_files, session_types = [], [], []\n",
        "\n",
        "### Define the upload UI element ###\n",
        "uploaded = files.upload()\n",
        "\n",
        "### Create Data dictonary and containers ###\n",
        "session_dict = {} # for storing by session type\n",
        "loaded_files = [] # for files\n",
        "\n",
        "### Loop through the uploaded files ###\n",
        "# This expects a zippped file exclusivly\n",
        "for name, data in uploaded.items():\n",
        "  with zipfile.ZipFile(io.BytesIO(data)) as zf: # use zipfile to \"unzip\"\n",
        "    for zi in zf.infolist(): # iterate through the zipped files\n",
        "\n",
        "\n",
        "      # skip non handled file types\n",
        "      # Skip non-csv/xlsx files\n",
        "      if not (zi.filename.endswith(\".csv\") or zi.filename.endswith(\".xlsx\")):\n",
        "        print(f\"Skipping: {zi.filename}\")\n",
        "        continue\n",
        "      print(f\"\\nProcessing: {zi.filename}\")\n",
        "\n",
        "      # define filetype\n",
        "      file_type = \"csv\" if zi.filename.endswith(\".csv\") else \"xlsx\"\n",
        "\n",
        "      # read the file in\n",
        "      file_data = zf.read(zi)\n",
        "\n",
        "      # write temporary file into memory\n",
        "      suffix = \".csv\" if file_type == \"csv\" else \".xlsx\"\n",
        "      with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=suffix, delete=False) as tmp:\n",
        "        tmp.write(file_data)\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "      # Define try catch statment and read into memory\n",
        "      try:\n",
        "        if file_type == \"csv\":\n",
        "          df = pd.read_csv(tmp_path)\n",
        "        else:\n",
        "          df = pd.read_excel(tmp_path)\n",
        "        #df[\"source_file\"] = zi.filename\n",
        "        session_type = extract_session_type(df, zi.filename)\n",
        "        # rewrite session\n",
        "        df[\"session_type\"] = session_type\n",
        "        print(session_type)\n",
        "\n",
        "        # Add to the data dictionary based on session type\n",
        "        if session_type not in session_dict:\n",
        "          session_dict[session_type] = []\n",
        "        session_dict[session_type].append(df)\n",
        "      except Exception as e: # throw error on failure to load\n",
        "        print(f\"Error loading {zi.filename}: {e}\")\n",
        "      finally: # remove the temporary path\n",
        "        os.remove(tmp_path)\n",
        "\n",
        "\n",
        "\n",
        "### Combine dataframes for each session ###\n",
        "# Convert each list of dfs into one merged dataframe per session type\n",
        "session_dfs = {k: pd.concat(v, ignore_index=True) for k, v in session_dict.items()}\n",
        "\n",
        "#session_dfs.pop('weight', None)\n",
        "#session_dfs.pop('bandit100', None)\n",
        "session_dfs.pop('beam', None)\n",
        "session_dfs.pop('si', None)\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in session_dfs.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(session_dfs[s])"
      ],
      "metadata": {
        "id": "8pJCTyGLzH2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin to clean data.\n",
        "Define what columns for what seesion types will be dropped. This should be based on nullness and should seek to keep as large an N as possible.\n",
        "\n",
        "Additionally, columns that are believe to add little to no context could be dropped."
      ],
      "metadata": {
        "id": "-dM7b0alrTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### BEGIN DATA CLEANING AND SCRUBBING #######\n",
        "\n",
        "\n",
        "# drop known problem columns\n",
        "all_drops = [\"filename\", \"filename_md\", \"Session_type\", \"match_status\",\n",
        "               \"Primary\", \"Autoexcluder\", \"Gene_ID\"]\n",
        "\n",
        "\n",
        "\n",
        "dfs_clean = aggregate_session_types(session_dfs)\n",
        "\n",
        "\n",
        "dfs_clean = clean_session_types(session_dfs=dfs_clean,\n",
        "                                all_drops=all_drops)\n",
        "\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in dfs_clean.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(dfs_clean[s])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WZ6yTNtP7X-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge and Prepare Columns\n",
        "\n",
        "##Begin the merge of the disperate types of metric data.\n",
        "\n",
        "Continues cleaning and data formatting including:\n",
        "\n",
        "*   Remove any rows that have any null values present.\n",
        "*   Calculate # of Days columns (if date times are present).\n"
      ],
      "metadata": {
        "id": "9sPLN44yyeL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Merge the list of dataframes #######\n",
        "\n",
        "# Define the features that will be merged on\n",
        "merge_keys = [\"Mouse_ID\", \"Gene\", \"Sex\", \"Genotype\"]\n",
        "\n",
        "#print(merge_keys)\n",
        "\n",
        "### Add prefixs for columns ###\n",
        "#Merge the dataframes and add a prefix to track where it came from\n",
        "# add prefixs to the dataframes based  on session type to track them\n",
        "prefixed_dfs = [\n",
        "    df.rename(columns=lambda c: f\"{s}_{c}\" if c not in merge_keys else c)\n",
        "    for s, df in dfs_clean.items()\n",
        "]\n",
        "\n",
        "### Merge the dataframes ###\n",
        "# Merge based on multiple grouping columns\n",
        "merged = reduce(\n",
        "    lambda left, right: pd.merge(left, right, on=merge_keys, how=\"outer\"),\n",
        "    prefixed_dfs\n",
        ")\n",
        "\n",
        "\n",
        "### Remove any rows that contain null values ###\n",
        "# Replace textual 'nan' with real NaN and assign\n",
        "replaced = merged.replace('nan', np.nan)\n",
        "# Keep only rows that have NO nulls in any column\n",
        "df_all = replaced[~replaced.isnull().any(axis=1)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "### Deal with the dates ###\n",
        "# by representing how old each mouse is (days) for various tasks\n",
        "dob_col = df_all.columns[df_all.columns.str.contains(\"DOB\")]\n",
        "if not dob_col.empty:\n",
        "  # Extract column\n",
        "  dob_col = dob_col[0]\n",
        "  print(f\"Found DOB Column: {dob_col} \\nCalculating days old for tasks\")\n",
        "  fed_starts = df_all.columns[df_all.columns.str.contains(\"FED_StartDate\")]\n",
        "\n",
        "  # Convert DOB column once to datetime\n",
        "  dob_dt = pd.to_datetime(df_all[dob_col], errors='coerce')\n",
        "\n",
        "  # Loop through each FED_StartDate column and calculate days difference\n",
        "  for fed_col in fed_starts:\n",
        "    new_col_name = f\"days_old_for_{fed_col}\"\n",
        "    df_all[new_col_name] = (\n",
        "        pd.to_datetime(df_all[fed_col], errors='coerce') - dob_dt\n",
        "        ).dt.days\n",
        "    print(f\"→ Created column: {new_col_name}\")\n",
        "\n",
        "\n",
        "### Remove any datetime columns ###\n",
        "datetime_cols = df_all.select_dtypes(include=['datetime64']).columns\n",
        "df_all.drop(columns = datetime_cols, inplace=True)\n",
        "\n",
        "\n",
        "### Check the data ###\n",
        "print(df_all.columns.values)\n",
        "df_prof(df_all)\n",
        "#df_all.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Save CSV ----------\n",
        "#csv_name = f\"PRmetrics_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "csv_name = f\"all_merged_dataframes.csv\"\n",
        "\n",
        "down_df = df_all.to_csv(csv_name, index=False)\n",
        "#display(HTML(f\"<b>✓ Saved PR metrics CSV to:</b> <code>{csv_name}</code>\"))\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "# Optional download button (works in Colab)\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name)}</code>…\"\n",
        "        gfiles.download(csv_name)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "JQrmiaETytxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepares data for Application to Multiple ML Models\n",
        "Should prepare data in theory for:\n",
        "*   PCA (Maybe)\n",
        "*   RandomForestModels\n",
        "\n",
        "This will include:\n",
        "1.   One-Hot Encoding\n",
        "2.   Split of Training and Test Data\n",
        "3.   Normalization of Data   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5grk05ssuHy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Prepare data for multiple ML approaches #######\n",
        "\n",
        "### Define the classifier for prediction ###\n",
        "pred_col = [\"Gene\"]\n",
        "\n",
        "### Define known categorical columns ###\n",
        "known_objects = [\"Sex\", \"Genotype\"]\n",
        "\n",
        "### Identify all FED and BEAM columns ###\n",
        "# are also categorical\n",
        "search_terms = ['FED3', 'BEAM']\n",
        "fed_beam_cols = [col for col in df_all.columns if any(term in col for term in search_terms)]\n",
        "known_objects.extend(fed_beam_cols)\n",
        "\n",
        "### Cast categorical columns ###\n",
        "df_all[known_objects] = df_all[known_objects].astype('object')\n",
        "\n",
        "# Define ID columns (Columns to Drop)\n",
        "id_cols = [\"Mouse_ID\"]\n",
        "id_cols.extend(pred_col)\n",
        "\n",
        "# Define X and y\n",
        "y = df_all[pred_col] # Currently not encoded\n",
        "X = df_all.drop(columns=id_cols)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# One-hot encode categorical features\n",
        "print(f\"One-hot encoding the following columns: {list(cat_cols)}\")\n",
        "X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "\n",
        "# Align columns between train and test\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Scale data\n",
        "# find numeric\n",
        "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "# find boolean\n",
        "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
        "\n",
        "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.fit_transform(X_test[num_cols])\n",
        "\n",
        "# Glimpse training data\n",
        "X_train.head()\n",
        "print(df_prof(X_train))"
      ],
      "metadata": {
        "id": "sdZFggMYA_mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2839420-a592-49ab-e010-5a2096322619"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoding the following columns: ['bandit80_BEAM', 'Genotype', 'Sex', 'bandit80_FED3', 'bandit80_FED_StartDate', 'bandit80_session_type', 'fr1_FED3', 'fr1_BEAM', 'fr1_session_type', 'bandit100_FED3', 'bandit100_BEAM', 'bandit100_FED_StartDate', 'bandit100_session_type', 'pr1_FED3', 'pr1_BEAM', 'pr1_DOB', 'pr1_Stim_Partner', 'pr1_session_type', 'weight_FED3', 'weight_BEAM', 'weight_Stim_Partner', 'weight_session_type']\n",
            "Number of rows: 100\n",
            "Number of columns: 888\n",
            "                              Column DataType  Percent_Null  Number_Unique\n",
            "0         bandit80_Win-stay_Bandit80  float64           0.0            100\n",
            "1       bandit80_Lose-shift_Bandit80  float64           0.0             93\n",
            "2     bandit80_PeakAccuracy_Bandit80  float64           0.0             86\n",
            "3    bandit80_Total_pellets_Bandit80  float64           0.0             74\n",
            "4      bandit80_Total_pokes_Bandit80  float64           0.0             88\n",
            "..                               ...      ...           ...            ...\n",
            "883          weight_Stim_Partner_M-1     bool           0.0              2\n",
            "884          weight_Stim_Partner_M-3     bool           0.0              2\n",
            "885          weight_Stim_Partner_M-4     bool           0.0              2\n",
            "886          weight_Stim_Partner_N-1     bool           0.0              2\n",
            "887          weight_Stim_Partner_N-4     bool           0.0              2\n",
            "\n",
            "[888 rows x 4 columns]\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format PCA Data"
      ],
      "metadata": {
        "id": "pdINveHO5r9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Format data into PCA ready format #######\n",
        "\n",
        "### Drop Keys and object columns ###\n",
        "pca_df = df_all.drop(columns = merge_keys)\n",
        "drops = pca_df.select_dtypes(include=['object', 'bool']).columns\n",
        "pca_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "features = pca_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(len(features))\n",
        "print(features)\n",
        "\n",
        "### Standardize the scale ###\n",
        "x = pca_df.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "pca_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "\n",
        "### Check the data ###\n",
        "df_prof(pca_df)\n",
        "pca_df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "DHf1ZMvH5uk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conduct PCA Plotting Colored by Various Categoricals"
      ],
      "metadata": {
        "id": "dcTvDzFARmrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### PCA Plotting #######\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "\n",
        "clustering_df_pca = pd.DataFrame(data = clustering_df_pca,\n",
        "                                 columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "print('Explained variability per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "#plt.figure()\n",
        "#plt.figure(figsize=(5,5))\n",
        "#plt.xticks(fontsize=12)\n",
        "#plt.yticks(fontsize=14)\n",
        "#plt.xlabel('Principal Component - 1',fontsize=20)\n",
        "#plt.ylabel('Principal Component - 2',fontsize=20)\n",
        "#plt.title(\"Principal Component Analysis Behavoirs\", fontsize=20)\n",
        "#targets = df_all['Gene'].unique()\n",
        "#colors = ['r', 'g', 'b', 'y', 'p']\n",
        "#for target, color in zip(targets,colors):\n",
        "#    indicesToKeep = df_all['Gene'] == target\n",
        "#    plt.scatter(clustering_df_pca.loc[indicesToKeep, 'principal component 1'],\n",
        "#                clustering_df_pca.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n",
        "#\n",
        "#plt.legend(targets,prop={'size': 8})\n",
        "\n",
        "\n",
        "# Colummns for coloring\n",
        "categorical_columns = ['Gene', 'Sex', 'Genotype']\n",
        "\n",
        "for cat_col in categorical_columns:\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.title(f\"PCA colored by {cat_col}\", fontsize=16)\n",
        "    plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
        "    plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
        "\n",
        "    ### Get unique groups for column ###\n",
        "    targets = df_all[cat_col].dropna().unique()\n",
        "\n",
        "    # Generate colors\n",
        "    cmap = plt.cm.get_cmap('viridis', len(targets))\n",
        "    colors = [cmap(i) for i in range(len(targets))]\n",
        "\n",
        "    ### Scatter for each group ###\n",
        "    for i, target in enumerate(targets):\n",
        "        indicesToKeep = df_all[cat_col] == target\n",
        "        plt.scatter(\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 1'],\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 2'],\n",
        "            color=colors[i],\n",
        "            s=50,\n",
        "            alpha=0.7,\n",
        "            edgecolors='k',\n",
        "            label=str(target)\n",
        "        )\n",
        "\n",
        "    plt.legend(title=cat_col, fontsize=9, title_fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hSKHHmV1Mtll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create K-Cluster Elbow Plot to Prepare Discovery of Clusters"
      ],
      "metadata": {
        "id": "6HVhBM98RXk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Explore how many K clusters #######\n",
        "\n",
        "# Initialize variables for Elbow Method\n",
        "inertia = []\n",
        "K_range = range(2, 11) # Testing 2 to 10 clusters\n",
        "\n",
        "# Iterate over potential numer of clusters\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=25).fit(pca_df)\n",
        "    inertia.append(kmeans.inertia_) # Within-cluster sum of squares\n",
        "# Plot the Elbow Method\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(K_range, inertia, 'o-', color='blue')\n",
        "plt.xlabel('Number of Clusters (k)', fontsize = 15)\n",
        "plt.ylabel('Within-Cluster Sum of Squares)', fontsize=15)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "zP38UtpYKhX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform the clustering using K from Above"
      ],
      "metadata": {
        "id": "-FTts6TZsbz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform k-means clustering and save\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(pca_df)\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "\n",
        "# Plot our pca data colored by clusters\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(clustering_df_pca[:, 0], clustering_df_pca[:, 1], c=labels,\n",
        "            cmap='viridis', s=30)\n",
        "plt.title('K-Means Clusters (PCA 2D Projection)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')"
      ],
      "metadata": {
        "id": "yrqfTcVvsiYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}